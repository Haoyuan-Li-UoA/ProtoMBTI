{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb873c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli962/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Encode & retrieve: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 325/325 [00:18<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é‡åŒ–å®Œæˆï¼š A_test_with_embeddings.json\n",
      "âœ… TopK éšæœº 4~6ï¼ˆå–å‰ k ä¸ªï¼‰å®Œæˆï¼š A_test_top4to6_final1.json\n"
     ]
    }
   ],
   "source": [
    "# encode_and_retrieve_topk.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, faiss, numpy as np, torch, random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= è·¯å¾„é…ç½® =========\n",
    "# å·²å»ºå¥½çš„å‘é‡åº“ï¼ˆç”± casebank_*_with_embeddings.json æ„å»ºï¼‰\n",
    "FAISS_INDEX_PATH = \"casebank_A_qwen_cls_cosine.index\"\n",
    "ID_MAP_PATH      = \"casebank_A_qwen_cls_idmap.json\"\n",
    "\n",
    "# å¾…å‘é‡åŒ–å¹¶æ£€ç´¢çš„æµ‹è¯•é›†\n",
    "TEST_INPUT          = \"picked_balanced_around30.json\"\n",
    "TEST_WITH_EMB_OUT   = \"A_test_with_embeddings.json\"\n",
    "TOPK_RESULTS_OUT    = \"A_test_top4to6_final1.json\"   # æ”¹ä¸ªåå­—ä»¥ç¤ºåŒºåˆ†\n",
    "\n",
    "# Qwen + LoRAï¼ˆä¸å»ºåº“æ—¶å®Œå…¨ä¸€è‡´ï¼‰\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_DIR = \"mbti_lora_qwen1.5b-split_kaggle_ckpt\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰è®¾å®šéšæœºç§å­ï¼Œä¿è¯å¤ç°ï¼›ä¸éœ€è¦å¯æ³¨é‡Šæ‰\n",
    "# random.seed(42)\n",
    "\n",
    "# ========= åŠ è½½ç´¢å¼• =========\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "id_map = json.load(open(ID_MAP_PATH, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# ========= åŠ è½½æ¨¡å‹ï¼ˆä¸å»ºåº“æ—¶ä¸€è‡´ï¼‰=========\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "cfg.num_labels = 16\n",
    "\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, config=cfg, trust_remote_code=True,\n",
    "    device_map={\"\": device}, low_cpu_mem_usage=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False).eval().to(device)\n",
    "\n",
    "# ========= å·¥å…·å‡½æ•° =========\n",
    "def pick_text(item: dict) -> str:\n",
    "    \"\"\"æŒ‰ä¼˜å…ˆçº§é€‰ä¸€ä¸ªå¯ç”¨äºç¼–ç çš„æ–‡æœ¬å­—æ®µã€‚æŒ‰ä½ çš„æ•°æ®ç»“æ„éœ€è¦å¯è°ƒæ•´é¡ºåºã€‚\"\"\"\n",
    "    for key in [\"posts_cleaned_for_embedding\", \"embed_text\", \"post_casebank\",\n",
    "                \"posts_cleaned\", \"posts\", \"text\", \"query_text\"]:\n",
    "        v = item.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return \"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_to_vec(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ç”¨ masked mean pooling æå¥å‘é‡ï¼ˆæ•´æ®µæ–‡æœ¬è¡¨å¾ï¼‰å¹¶åš L2 å½’ä¸€åŒ–ï¼Œ\n",
    "    ä¸ç´¢å¼•ç«¯ IndexFlatIPï¼ˆå†…ç§¯â‰ˆä½™å¼¦ï¼‰ä¸€è‡´ã€‚\n",
    "    \"\"\"\n",
    "    ins = tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n",
    "    out = model.base_model.model(**ins, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "    last_hidden = out.hidden_states[-1]                 # (B, T, H)\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1).float()  # (B, T, 1)\n",
    "\n",
    "    masked = last_hidden * mask                         # zero out pads\n",
    "    sum_hidden = masked.sum(dim=1)                      # (B, H)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1.0)            # (B, 1)\n",
    "    vec = sum_hidden / lengths                          # (B, H)\n",
    "\n",
    "    vec = F.normalize(vec, p=2, dim=1)                  # L2 å½’ä¸€åŒ–\n",
    "    return vec.squeeze(0).float().cpu().numpy()         # (H,)\n",
    "\n",
    "def search_topk(vec: np.ndarray, k: int):\n",
    "    D, I = index.search(vec.reshape(1, -1), k)\n",
    "    return [{\"score\": float(s), **id_map[str(int(i))]} for s, i in zip(D[0], I[0])]\n",
    "\n",
    "def filter_self_hits(query_text: str, hits: list, k: int):\n",
    "    \"\"\"å¯é€‰ï¼šè¿‡æ»¤ä¸æŸ¥è¯¢æ–‡æœ¬å®Œå…¨ç›¸åŒçš„è¿”å›ï¼Œé¿å…â€œå‘½ä¸­è‡ªå·±â€\"\"\"\n",
    "    def norm(s): return \" \".join((s or \"\").lower().split())\n",
    "    qt = norm(query_text)\n",
    "    keep = [h for h in hits if norm(h.get(\"post_casebank\", \"\")) != qt]\n",
    "    return keep[:k]\n",
    "\n",
    "# ========= è¯»å–æµ‹è¯•é›†ã€ç¼–ç ã€æ£€ç´¢ =========\n",
    "test = json.load(open(TEST_INPUT, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "with_emb, results = [], []\n",
    "for it in tqdm(test, desc=\"Encode & retrieve\"):\n",
    "    text = pick_text(it)\n",
    "    if not text:\n",
    "        # æ²¡æœ‰å¯ç”¨æ–‡æœ¬å°±è®°ç©º\n",
    "        it_out = dict(it); it_out[\"embedding\"] = None\n",
    "        with_emb.append(it_out)\n",
    "        results.append({\"type\": it.get(\"type\",\"\"), \"query_text\": \"\", \"topk_cases\": []})\n",
    "        continue\n",
    "\n",
    "    emb = encode_text_to_vec(text)              # (H,)\n",
    "    it_out = dict(it); it_out[\"embedding\"] = emb.tolist()\n",
    "    with_emb.append(it_out)\n",
    "\n",
    "    # â€”â€” è¿™é‡Œéšæœºå†³å®š k = 4 / 5 / 6ï¼Œå¹¶å–å‰ k ä¸ªæœ€ç›¸ä¼¼ â€”â€” #\n",
    "    k_dynamic = random.randint(4, 6)\n",
    "    topk = search_topk(emb, k=k_dynamic)\n",
    "    topk = filter_self_hits(text, topk, k_dynamic)   # ä¸æ‹…å¿ƒå‘½ä¸­è‡ªå·±å¯æ³¨é‡Šæ‰\n",
    "\n",
    "    results.append({\n",
    "        \"type\": it.get(\"type\",\"\"),\n",
    "        \"query_text\": text,\n",
    "        \"topk_cases\": topk\n",
    "    })\n",
    "\n",
    "# ========= è½ç›˜ =========\n",
    "json.dump(with_emb, open(TEST_WITH_EMB_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "json.dump(results, open(TOPK_RESULTS_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åŒ–å®Œæˆï¼š\", TEST_WITH_EMB_OUT)\n",
    "print(\"âœ… TopK éšæœº 4~6ï¼ˆå–å‰ k ä¸ªï¼‰å®Œæˆï¼š\", TOPK_RESULTS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d88d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Encode & retrieve (Random-proto GLOBAL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 325/325 [00:16<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é‡åŒ–å®Œæˆï¼š A_test_with_embeddings.json\n",
      "âœ… Random-proto (å…¨å±€éšæœºæŠ½ 3 ä¸ª) å®Œæˆï¼š A_test_random_global3_final1.json\n"
     ]
    }
   ],
   "source": [
    "# encode_and_retrieve_random_proto_global.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, faiss, numpy as np, torch, random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= è·¯å¾„é…ç½® =========\n",
    "FAISS_INDEX_PATH = \"casebank_A_qwen_cls_cosine.index\"   # è¿˜æ˜¯éœ€è¦åŠ è½½ä¸€ä¸‹ï¼ˆembeddingä¸€è‡´ï¼‰\n",
    "ID_MAP_PATH      = \"casebank_A_qwen_cls_idmap.json\"     # casebank æ˜ å°„è¡¨\n",
    "\n",
    "TEST_INPUT          = \"picked_balanced_around30.json\"\n",
    "TEST_WITH_EMB_OUT   = \"A_test_with_embeddings.json\"\n",
    "TOPK_RESULTS_OUT    = \"A_test_random_global3_final1.json\"   # Random-proto (å…¨å±€éšæœº 3)\n",
    "\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_DIR = \"mbti_lora_qwen1.5b-split_kaggle_ckpt\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# å¯é€‰ï¼šè®¾ç½®éšæœºç§å­ä¿è¯å¤ç°\n",
    "# random.seed(42)\n",
    "\n",
    "# ========= åŠ è½½ casebank =========\n",
    "id_map = json.load(open(ID_MAP_PATH, \"r\", encoding=\"utf-8\"))\n",
    "casebank = list(id_map.values())   # å–å‡ºæ‰€æœ‰æ ·æœ¬ï¼Œåé¢ç›´æ¥éšæœºæŠ½\n",
    "\n",
    "# ========= åŠ è½½æ¨¡å‹ =========\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "cfg.num_labels = 16\n",
    "\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, config=cfg, trust_remote_code=True,\n",
    "    device_map={\"\": device}, low_cpu_mem_usage=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False).eval().to(device)\n",
    "\n",
    "# ========= å·¥å…·å‡½æ•° =========\n",
    "def pick_text(item: dict) -> str:\n",
    "    for key in [\"posts_cleaned_for_embedding\", \"embed_text\", \"post_casebank\",\n",
    "                \"posts_cleaned\", \"posts\", \"text\", \"query_text\"]:\n",
    "        v = item.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return \"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_to_vec(text: str) -> np.ndarray:\n",
    "    ins = tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n",
    "    out = model.base_model.model(**ins, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "    last_hidden = out.hidden_states[-1]\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1).float()\n",
    "\n",
    "    masked = last_hidden * mask\n",
    "    sum_hidden = masked.sum(dim=1)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1.0)\n",
    "    vec = sum_hidden / lengths\n",
    "\n",
    "    vec = F.normalize(vec, p=2, dim=1)\n",
    "    return vec.squeeze(0).float().cpu().numpy()\n",
    "\n",
    "# ========= ä¸»æµç¨‹ =========\n",
    "test = json.load(open(TEST_INPUT, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "with_emb, results = [], []\n",
    "for it in tqdm(test, desc=\"Encode & retrieve (Random-proto GLOBAL)\"):\n",
    "    text = pick_text(it)\n",
    "    if not text:\n",
    "        it_out = dict(it); it_out[\"embedding\"] = None\n",
    "        with_emb.append(it_out)\n",
    "        results.append({\"type\": it.get(\"type\",\"\"), \"query_text\": \"\", \"topk_cases\": []})\n",
    "        continue\n",
    "\n",
    "    emb = encode_text_to_vec(text)\n",
    "    it_out = dict(it); it_out[\"embedding\"] = emb.tolist()\n",
    "    with_emb.append(it_out)\n",
    "\n",
    "    # === å…¨å±€éšæœºæŠ½ 3 ä¸ª case ===\n",
    "    topk = random.sample(casebank, 3) if len(casebank) >= 3 else casebank\n",
    "\n",
    "    results.append({\n",
    "        \"type\": it.get(\"type\",\"\"),\n",
    "        \"query_text\": text,\n",
    "        \"topk_cases\": topk\n",
    "    })\n",
    "\n",
    "# ========= è½ç›˜ =========\n",
    "json.dump(with_emb, open(TEST_WITH_EMB_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "json.dump(results, open(TOPK_RESULTS_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åŒ–å®Œæˆï¼š\", TEST_WITH_EMB_OUT)\n",
    "print(\"âœ… Random-proto (å…¨å±€éšæœºæŠ½ 3 ä¸ª) å®Œæˆï¼š\", TOPK_RESULTS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f768973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode casebank with Qwen-base: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [02:39<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç´¢å¼•å·²å†™å…¥: casebank_qwenbase_cosine.index\n",
      "âœ… id_map å·²å†™å…¥: casebank_qwenbase_idmap.json\n",
      "ğŸ“¦ å‘é‡æ•°: 27224, ç»´åº¦: 1536\n"
     ]
    }
   ],
   "source": [
    "# build_casebank_index_qwen_base.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, faiss, numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ======= è·¯å¾„é…ç½®ï¼ˆæŒ‰ä½ æœ¬åœ°ä¿®æ”¹ï¼‰ =======\n",
    "ID_MAP_IN          = \"casebank_A_qwen_cls_idmap.json\"   # ç°æœ‰çš„ casebank å…ƒæ•°æ®ï¼ˆåªè¯»ï¼‰\n",
    "OUT_INDEX_PATH     = \"casebank_qwenbase_cosine.index\"   # æ–°ç´¢å¼•ï¼ˆQwen åŸºåº§å‘é‡ï¼‰\n",
    "OUT_ID_MAP_COPY    = \"casebank_qwenbase_idmap.json\"     # å¤åˆ¶ä¸€ä»½ id_mapï¼ˆä¿æŒä¸€è‡´ï¼‰\n",
    "\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"              # çº¯åŸºåº§ï¼Œä¸åŠ è½½ LoRA\n",
    "MAX_LEN     = 512\n",
    "BATCH_SIZE  = 64\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ======= åŠ è½½ Qwen åŸºåº§ =======\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    BASE_MODEL, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else None\n",
    ").eval().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts):\n",
    "    ins = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN).to(device)\n",
    "    out = model(**ins, output_hidden_states=True, return_dict=True)\n",
    "    last = out.last_hidden_state                    # (B,T,H)\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1).float()\n",
    "    vec = (last * mask).sum(1) / mask.sum(1).clamp(min=1e-6)   # mean pooling\n",
    "    vec = F.normalize(vec, p=2, dim=1)                          # L2 å½’ä¸€åŒ– â†’ ä½™å¼¦\n",
    "    return vec.detach().cpu().numpy().astype(\"float32\")         # (B,H)\n",
    "\n",
    "# ======= è¯»å– casebank å¹¶ç¼–ç  =======\n",
    "id_map = json.load(open(ID_MAP_IN, \"r\", encoding=\"utf-8\"))\n",
    "# ä¿æŒé¡ºåºç¨³å®š\n",
    "items = [id_map[k] for k in sorted(id_map.keys(), key=lambda x: int(x))]\n",
    "\n",
    "vecs = []\n",
    "for i in tqdm(range(0, len(items), BATCH_SIZE), desc=\"Encode casebank with Qwen-base\"):\n",
    "    texts = [ (it.get(\"post_casebank\") or it.get(\"post\") or \"\") for it in items[i:i+BATCH_SIZE] ]\n",
    "    vecs.append(embed_texts(texts))\n",
    "vecs = np.vstack(vecs)  # (N,H), float32\n",
    "\n",
    "# ======= å»º Faiss ç´¢å¼•ï¼ˆcosine = L2 + IPï¼‰ =======\n",
    "index = faiss.IndexFlatIP(vecs.shape[1])\n",
    "index.add(vecs)\n",
    "faiss.write_index(index, OUT_INDEX_PATH)\n",
    "\n",
    "# å¤åˆ¶ä¸€ä»½ id_map ä»¥é…å¥—æ–°ç´¢å¼•\n",
    "json.dump(id_map, open(OUT_ID_MAP_COPY, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ç´¢å¼•å·²å†™å…¥: {OUT_INDEX_PATH}\")\n",
    "print(f\"âœ… id_map å·²å†™å…¥: {OUT_ID_MAP_COPY}\")\n",
    "print(f\"ğŸ“¦ å‘é‡æ•°: {vecs.shape[0]}, ç»´åº¦: {vecs.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0703242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode & retrieve Â· Top-k (Qwen-base): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 325/325 [00:06<00:00, 49.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é‡åŒ–å®Œæˆï¼š QwenBase_test_with_embeddings.json\n",
      "âœ… æ£€ç´¢å®Œæˆï¼ˆTop-k (Qwen-base)ï¼‰: QwenBase_test_topk3_final1.json\n"
     ]
    }
   ],
   "source": [
    "# encode_and_retrieve_topk_qwen_base.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, faiss, numpy as np, torch, random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= è·¯å¾„é…ç½® =========\n",
    "FAISS_INDEX_PATH   = \"casebank_qwenbase_cosine.index\"   # ç”¨â‘ ç”Ÿæˆçš„æ–°ç´¢å¼•\n",
    "ID_MAP_PATH        = \"casebank_qwenbase_idmap.json\"     # ä¸æ–°ç´¢å¼•é…å¥—çš„ id_map\n",
    "\n",
    "TEST_INPUT         = \"picked_balanced_around30.json\"\n",
    "TEST_WITH_EMB_OUT  = \"QwenBase_test_with_embeddings.json\"\n",
    "TOPK_RESULTS_OUT   = \"QwenBase_test_topk3_final1.json\"         # e.g., top-3\n",
    "K                  = 3                                  # è¿™é‡Œé…ä½ è¦çš„ kï¼ˆbaseline=3ï¼‰\n",
    "\n",
    "# è‹¥è¦ Random-protoï¼ˆå…¨åº“éšæœº 3 ä¸ªï¼‰ï¼ŒæŠŠ MODE æ”¹ä¸º \"random_global\"\n",
    "# è‹¥è¦éšæœº 4â€“6ï¼ˆå–å‰ k ä¸ªæœ€ç›¸ä¼¼ï¼‰ï¼ŒæŠŠ MODE æ”¹ä¸º \"topk_4to6\"\n",
    "MODE               = \"topk\"  # \"topk\" | \"topk_4to6\" | \"random_global\"\n",
    "\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MAX_LEN     = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========= åŠ è½½ç´¢å¼• & id_map =========\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "id_map = json.load(open(ID_MAP_PATH, \"r\", encoding=\"utf-8\"))\n",
    "casebank_all = list(id_map.values())  # ç»™ random_global ç”¨\n",
    "\n",
    "# ========= åŠ è½½ Qwen åŸºåº§ =========\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    BASE_MODEL, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else None\n",
    ").eval().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_to_vec(text: str) -> np.ndarray:\n",
    "    ins = tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n",
    "    out = model(**ins, output_hidden_states=True, return_dict=True)\n",
    "    last = out.last_hidden_state\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1).float()\n",
    "    vec = (last * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "    vec = F.normalize(vec, p=2, dim=1)\n",
    "    return vec.squeeze(0).detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def search_topk(vec: np.ndarray, k: int):\n",
    "    D, I = index.search(vec.reshape(1, -1), k)\n",
    "    return [{\"score\": float(s), **id_map[str(int(i))]} for s, i in zip(D[0], I[0])]\n",
    "\n",
    "def filter_self_hits(query_text: str, hits: list, k: int = None):\n",
    "    def norm(s): return \" \".join((s or \"\").lower().split())\n",
    "    qt = norm(query_text)\n",
    "    keep = [h for h in hits if norm(h.get(\"post_casebank\", \"\")) != qt]\n",
    "    return keep if k is None else keep[:k]\n",
    "\n",
    "# ========= è¯»å–æµ‹è¯•é›†ã€ç¼–ç ã€æ£€ç´¢ =========\n",
    "test = json.load(open(TEST_INPUT, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "with_emb, results = [], []\n",
    "desc = {\n",
    "    \"topk\": \"Top-k (Qwen-base)\",\n",
    "    \"topk_4to6\": \"Top-k random 4â€“6 (Qwen-base)\",\n",
    "    \"random_global\": \"Random-proto global 3 (Qwen-base)\"\n",
    "}[MODE]\n",
    "\n",
    "for it in tqdm(test, desc=f\"Encode & retrieve Â· {desc}\"):\n",
    "    # å–æ–‡æœ¬\n",
    "    text = None\n",
    "    for key in [\"posts_cleaned_for_embedding\", \"embed_text\", \"post_casebank\",\n",
    "                \"posts_cleaned\", \"posts\", \"text\", \"query_text\"]:\n",
    "        v = it.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            text = v\n",
    "            break\n",
    "    if not text:\n",
    "        it_out = dict(it); it_out[\"embedding\"] = None\n",
    "        with_emb.append(it_out)\n",
    "        results.append({\"type\": it.get(\"type\",\"\"), \"query_text\": \"\", \"topk_cases\": []})\n",
    "        continue\n",
    "\n",
    "    emb = encode_text_to_vec(text)\n",
    "    it_out = dict(it); it_out[\"embedding\"] = emb.tolist()\n",
    "    with_emb.append(it_out)\n",
    "\n",
    "    # ä¸‰ç§æ¨¡å¼\n",
    "    if MODE == \"topk\":\n",
    "        topk = filter_self_hits(text, search_topk(emb, k=K), K)\n",
    "\n",
    "    elif MODE == \"topk_4to6\":\n",
    "        k_dynamic = random.randint(4, 6)\n",
    "        topk = filter_self_hits(text, search_topk(emb, k=k_dynamic), k_dynamic)\n",
    "\n",
    "    elif MODE == \"random_global\":\n",
    "        # å…¨åº“éšæœº 3 ä¸ªï¼ˆä¸ä½ ä¹‹å‰çš„ Random-proto éœ€æ±‚ä¸€è‡´ï¼‰\n",
    "        pool = casebank_all\n",
    "        topk = random.sample(pool, 3) if len(pool) >= 3 else pool\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MODE: {MODE}\")\n",
    "\n",
    "    results.append({\n",
    "        \"type\": it.get(\"type\",\"\"),\n",
    "        \"query_text\": text,\n",
    "        \"topk_cases\": topk\n",
    "    })\n",
    "\n",
    "# ========= è½ç›˜ =========\n",
    "json.dump(with_emb, open(TEST_WITH_EMB_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "json.dump(results, open(TOPK_RESULTS_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åŒ–å®Œæˆï¼š\", TEST_WITH_EMB_OUT)\n",
    "print(f\"âœ… æ£€ç´¢å®Œæˆï¼ˆ{desc}ï¼‰: {TOPK_RESULTS_OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
