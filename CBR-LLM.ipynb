{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3af157-1d42-46dc-aff9-223a066ea1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ä¸‹è½½å¿…è¦èµ„æº\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# MBTI ç±»å‹å…³é”®è¯ï¼ˆå¤§å†™åŒ¹é…ï¼‰\n",
    "mbti_types = {\n",
    "    'INTJ', 'INTP', 'ENTJ', 'ENTP',\n",
    "    'INFJ', 'INFP', 'ENFJ', 'ENFP',\n",
    "    'ISTJ', 'ISFJ', 'ESTJ', 'ESFJ',\n",
    "    'ISTP', 'ISFP', 'ESTP', 'ESFP'\n",
    "}\n",
    "\n",
    "# æ·±åº¦æ¸…æ´—ï¼šç”¨äº posts_cleaned\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = text.replace('|', ' ')\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# è½»åº¦æ¸…æ´—ï¼šç”¨äº post_casebankï¼ˆä»…å»é“¾æ¥ï¼‰\n",
    "def clean_casebank_keep_format(text):\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ä¸»æ¸…æ´—é€»è¾‘\n",
    "def preprocess_mbti_json(input_json_path, output_json_path=None, output_csv_path=None):\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cleaned_data = []\n",
    "    MIN_TOKEN_LEN = 10  # è¯æ•°ä¸‹é™\n",
    "\n",
    "    for item in tqdm(data, desc=\"ğŸ§¹ Cleaning MBTI posts\"):\n",
    "        raw_posts = item.get(\"posts\", \"\")\n",
    "        segments = raw_posts.strip(\"'\").split(\"|||\")\n",
    "\n",
    "        cleaned_segments = []\n",
    "        casebank_segments = []\n",
    "\n",
    "        for seg in segments:\n",
    "            seg = seg.strip()\n",
    "            has_url = 'http' in seg or 'www.' in seg\n",
    "            token_len = len(nltk.word_tokenize(seg))\n",
    "            contains_mbti = any(t in seg.upper() for t in mbti_types)\n",
    "\n",
    "            # ä¸¢å¼ƒè§„åˆ™ç»Ÿä¸€ï¼šçŸ­ + URL + MBTI æ ‡ç­¾ â†’ ä¸¢\n",
    "            keep = not has_url and token_len > MIN_TOKEN_LEN and not contains_mbti\n",
    "\n",
    "            if keep:\n",
    "                cleaned = clean_text(seg)\n",
    "                casebank_cleaned = clean_casebank_keep_format(seg)\n",
    "\n",
    "                if cleaned:\n",
    "                    cleaned_segments.append(cleaned)\n",
    "                if casebank_cleaned:\n",
    "                    casebank_segments.append(casebank_cleaned)\n",
    "\n",
    "        cleaned_data.append({\n",
    "            \"type\": item.get(\"type\"),\n",
    "            \"posts\": raw_posts,\n",
    "            \"posts_cleaned\":item.get(\"posts_cleaned\"),\n",
    "            \"post_casebank\": \" \".join(casebank_segments)\n",
    "        })\n",
    "\n",
    "    # ä¿å­˜ JSON\n",
    "    if output_json_path:\n",
    "        os.makedirs(os.path.dirname(output_json_path) or \".\", exist_ok=True)\n",
    "        with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… JSON saved to: {output_json_path}\")\n",
    "\n",
    "    # æç¤º CSV è¾“å‡º\n",
    "    if output_csv_path:\n",
    "        print(\"âš ï¸ CSVå»ºè®®åªå¯¼å‡ºå­—æ®µï¼štype, posts_cleaned, post_casebank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23163b-6155-4418-9ff9-2b58e97b98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_mbti_json(\n",
    "    input_json_path=\"extended_mbti_dataset_v17.json\",\n",
    "    output_csv_path=\"extended_mbti_v17_cleaned.csv\",\n",
    "    output_json_path=\"casebank_mbti.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b017ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora æ¸…ç†éƒ¨åˆ†\n",
    "import json, re, hashlib\n",
    "from collections import Counter\n",
    "# from typing import Optional  # å¦‚æœæ˜¯ py3.8/3.9ï¼Œæ‰“å¼€å¹¶æŠŠä¸‹é¢çš„è¿”å›ç±»å‹æ”¹æˆ Optional[dict]\n",
    "\n",
    "MBTI_16 = {\"INTJ\",\"INTP\",\"ENTJ\",\"ENTP\",\"INFJ\",\"INFP\",\"ENFJ\",\"ENFP\",\n",
    "           \"ISTJ\",\"ISFJ\",\"ESTJ\",\"ESFJ\",\"ISTP\",\"ISFP\",\"ESTP\",\"ESFP\"}\n",
    "\n",
    "def light_clean(x: str) -> str:\n",
    "    if not x: return \"\"\n",
    "    x = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', x)\n",
    "    x = x.replace('|',' ')\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    return x\n",
    "\n",
    "def build_case_row(r: dict, max_chars: int = 1200) -> dict:  # å¦‚æœä½ç‰ˆæœ¬ï¼Œå†™ Optional[dict]\n",
    "    t = str(r.get(\"type\",\"\")).upper().strip()\n",
    "    if t not in MBTI_16: \n",
    "        return None\n",
    "\n",
    "    base = light_clean(r.get(\"posts_cleaned\") or r.get(\"posts\") or \"\")\n",
    "    sem  = light_clean(r.get(\"semantic_view\",\"\"))\n",
    "    sen  = light_clean(r.get(\"sentiment_view\",\"\"))\n",
    "    lin  = light_clean(r.get(\"linguistic_view\",\"\"))\n",
    "\n",
    "    if not base:\n",
    "        return None\n",
    "\n",
    "    meta = []\n",
    "    if sem: meta.append(f\"[Semantic] {sem}\")\n",
    "    if sen: meta.append(f\"[Sentiment] {sen}\")\n",
    "    if lin: meta.append(f\"[Linguistic] {lin}\")\n",
    "    post_casebank = base if not meta else f\"{base} \" + \" \".join(meta)\n",
    "\n",
    "    if len(post_casebank) > max_chars:\n",
    "        post_casebank = post_casebank[:max_chars].rstrip() + \"â€¦\"\n",
    "\n",
    "    return {\"type\": t, \"post_casebank\": post_casebank, \"embed_text\": base}\n",
    "\n",
    "def make_casebank(in_file: str, out_file: str, max_chars: int = 1200, dedup: bool = True):\n",
    "    data = json.load(open(in_file, \"r\", encoding=\"utf-8\"))\n",
    "    out, seen = [], set()\n",
    "\n",
    "    for r in data:\n",
    "        row = build_case_row(r, max_chars=max_chars)\n",
    "        if not row: \n",
    "            continue\n",
    "        if dedup:\n",
    "            h = hashlib.md5(row[\"embed_text\"].encode(\"utf-8\")).hexdigest()\n",
    "            if h in seen:\n",
    "                continue\n",
    "            seen.add(h)\n",
    "        out.append(row)\n",
    "\n",
    "    json.dump(out, open(out_file, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "    cnt = Counter([x[\"type\"] for x in out])\n",
    "    total = len(out)\n",
    "    print(f\"âœ… Casebank å®Œæˆï¼š{total} æ¡ â†’ {out_file}\")\n",
    "    print(\"ğŸ“Š ç±»å‹åˆ†å¸ƒï¼š\")\n",
    "    for k in sorted(cnt):\n",
    "        print(f\"  {k}: {cnt[k]} ({cnt[k]/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb185ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH  = \"pandora_eval_80.json\"              # ä½ çš„æ–‡ä»¶\n",
    "OUTPUT_PATH = \"casebank_pandora_eval_80.json\"     # è¾“å‡º\n",
    "\n",
    "make_casebank(INPUT_PATH, OUTPUT_PATH, max_chars=1200, dedup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_DIR = \"mbti_lora_qwen1.5b-split_kaggle_ckpt\"   # âœ… æŒ‡å‘åŒ…å« adapter_config.json çš„ç›®å½•\n",
    "\n",
    "assert os.path.exists(os.path.join(ADAPTER_DIR, \"adapter_config.json\")), \"è·¯å¾„ä¸å¯¹ï¼Œæ‰¾ä¸åˆ° adapter_config.json\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "cfg.num_labels = 16\n",
    "\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, config=cfg, trust_remote_code=True,\n",
    "    device_map={\"\": \"cuda:0\"}, low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False).eval()\n",
    "print(\"âœ… LoRA é€‚é…å™¨åŠ è½½æˆåŠŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_embeddings_qwen_meanpool.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ===== è·¯å¾„é…ç½® =====\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_DIR = \"mbti_lora_qwen1.5b-split_kaggle_ckpt\"\n",
    "INPUT_FILE  = \"train.json\"\n",
    "OUTPUT_FILE = \"casebank_A_train_80_with_embeddings.json\"\n",
    "\n",
    "# è®¾å¤‡\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device_map = {\"\": \"cuda:0\"} if use_cuda else None\n",
    "\n",
    "# ===== tokenizer & model =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "config.num_labels = 16\n",
    "\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False)\n",
    "model.eval().to(device)\n",
    "\n",
    "# ===== å¥å‘é‡æå–ï¼ˆmasked mean poolingï¼›æ¨èï¼‰ =====\n",
    "@torch.no_grad()\n",
    "def get_text_embedding(text: str, max_len: int = 512) -> list[float]:\n",
    "    ins = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    out = model.base_model.model(\n",
    "        **ins, output_hidden_states=True, use_cache=False, return_dict=True\n",
    "    )\n",
    "    last_hidden = out.hidden_states[-1]          # (B, T, H)\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1)   # (B, T, 1)\n",
    "\n",
    "    # masked mean pooling\n",
    "    masked = last_hidden * mask                  # zero out pads\n",
    "    sum_hidden = masked.sum(dim=1)               # (B, H)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1)       # (B, 1)\n",
    "    emb = sum_hidden / lengths                   # (B, H)\n",
    "\n",
    "    # L2 å½’ä¸€åŒ–ï¼ˆå’Œ FAISS é‡Œä¸€è‡´ï¼šä½™å¼¦=å†…ç§¯ï¼‰\n",
    "    emb = F.normalize(emb, p=2, dim=1)           # (B, H)\n",
    "    return emb.squeeze(0).cpu().tolist()\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰æœ€å token pooling ç‰ˆæœ¬ï¼š\n",
    "# def get_text_embedding(text: str, max_len: int = 512) -> list[float]:\n",
    "#     ins = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_len).to(device)\n",
    "#     out = model.base_model.model(**ins, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "#     last_hidden = out.hidden_states[-1]                # (B, T, H)\n",
    "#     idx = ins[\"attention_mask\"].sum(dim=1) - 1         # (B,)\n",
    "#     emb = last_hidden[torch.arange(last_hidden.size(0)), idx]  # (B, H)\n",
    "#     emb = F.normalize(emb, p=2, dim=1)\n",
    "#     return emb.squeeze(0).cpu().tolist()\n",
    "\n",
    "# ===== å¤„ç† casebank =====\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "output = []\n",
    "for item in tqdm(data, desc=\"Extracting sentence embeddings\"):\n",
    "    # ä»ä½¿ç”¨ä½ æ„å»º casebank æ—¶çš„å±•ç¤ºå­—æ®µ\n",
    "    text = item.get(\"post_casebank\") or item.get(\"embed_text\") or item.get(\"posts_cleaned\") or item.get(\"posts\") or \"\"\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    emb = get_text_embedding(text)\n",
    "    item[\"embedding\"] = emb\n",
    "    output.append(item)\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45305b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_faiss_from_casebank.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, re\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH       = \"casebank_A_train_80_with_embeddings.json\"\n",
    "FAISS_INDEX_PATH= \"casebank_A_qwen_cls_cosine.index\"\n",
    "ID_MAP_PATH     = \"casebank_A_qwen_cls_idmap.json\"\n",
    "\n",
    "def light_clean(x: str) -> str:\n",
    "    if not x: return \"\"\n",
    "    x = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', x)\n",
    "    x = x.replace('|',' ')\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    return x\n",
    "\n",
    "def ensure_post_casebank(item: dict, max_chars: int = 1200) -> str:\n",
    "    \"\"\"ä¸‡ä¸€ä¸Šä¸€æ­¥æ²¡å†™å…¥ post_casebankï¼Œè¿™é‡Œå…œåº•å†æ„å»ºä¸€æ¬¡ã€‚\"\"\"\n",
    "    if \"post_casebank\" in item and item[\"post_casebank\"].strip():\n",
    "        return item[\"post_casebank\"]\n",
    "\n",
    "    base = light_clean(item.get(\"embed_text\") or item.get(\"posts_cleaned\") or item.get(\"posts\") or \"\")\n",
    "    sem  = light_clean(item.get(\"semantic_view\",\"\"))\n",
    "    sen  = light_clean(item.get(\"sentiment_view\",\"\"))\n",
    "    lin  = light_clean(item.get(\"linguistic_view\",\"\"))\n",
    "    meta = []\n",
    "    if sem: meta.append(f\"[Semantic] {sem}\")\n",
    "    if sen: meta.append(f\"[Sentiment] {sen}\")\n",
    "    if lin: meta.append(f\"[Linguistic] {lin}\")\n",
    "    post_casebank = base if not meta else f\"{base} \" + \" \".join(meta)\n",
    "    if len(post_casebank) > max_chars:\n",
    "        post_casebank = post_casebank[:max_chars].rstrip() + \"â€¦\"\n",
    "    return post_casebank\n",
    "\n",
    "# === åŠ è½½ ===\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "embs, id_map = [], {}\n",
    "for idx, item in enumerate(raw):\n",
    "    emb = np.array(item[\"embedding\"], dtype=np.float32)\n",
    "    embs.append(emb)\n",
    "    id_map[idx] = {\n",
    "        \"post_casebank\": ensure_post_casebank(item),\n",
    "        \"type\": item.get(\"type\", \"\")\n",
    "    }\n",
    "\n",
    "embs = np.vstack(embs).astype(\"float32\")\n",
    "faiss.normalize_L2(embs)                # å½’ä¸€åŒ–ä½¿å†…ç§¯=ä½™å¼¦\n",
    "\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embs)\n",
    "\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "with open(ID_MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åº“å·²æ„å»º:\", FAISS_INDEX_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity_check_and_search.py\n",
    "import json, faiss, numpy as np\n",
    "\n",
    "FAISS_INDEX_PATH = \"casebank_A_qwen_cls_cosine.index\"\n",
    "ID_MAP_PATH      = \"casebank_A_qwen_cls_idmap.json\"\n",
    "EMB_DATA_PATH    = \"casebank_A_train_80_with_embeddings.json\"  # å°±æ˜¯ä½ å»ºåº“æ—¶çš„é‚£ä¸ª JSON\n",
    "\n",
    "# 1) åŠ è½½ç´¢å¼•å’Œ id_map\n",
    "index  = faiss.read_index(FAISS_INDEX_PATH)\n",
    "with open(ID_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    id_map = json.load(f)\n",
    "\n",
    "print(\"index.ntotal =\", index.ntotal, \" | id_map size =\", len(id_map))\n",
    "\n",
    "# 2) è¯»å–åŒä¸€ä¸ª embeddings æ•°æ®æ–‡ä»¶ï¼Œæ‹¿ä¸€ä¸ªå‘é‡å½“ query\n",
    "with open(EMB_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "assert len(data) > 0, \"embeddings æ•°æ®ä¸ºç©º\"\n",
    "k = 10 if len(data) > 10 else 0                    # é€‰ç¬¬ k ä¸ªæ ·æœ¬\n",
    "q = np.array(data[k][\"embedding\"], dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "# å¦‚æœä½ æ„å»ºç´¢å¼•å‰åšäº† L2 å½’ä¸€åŒ–ï¼Œè¿™é‡Œæœ€å¥½å†å½’ä¸€åŒ–ä¸€æ¬¡ï¼ˆä¿é™©ï¼‰\n",
    "faiss.normalize_L2(q)\n",
    "\n",
    "# 3) æ£€ç´¢ top-5\n",
    "D, I = index.search(q, 5)\n",
    "\n",
    "# 4) æ‰“å°ç»“æœ\n",
    "for s, idx in zip(D[0], I[0]):\n",
    "    info = id_map[str(int(idx))]                   # id_map çš„ key æ˜¯å­—ç¬¦ä¸²\n",
    "    print(f\"{s:.4f}\", info[\"type\"], \"|\", info[\"post_casebank\"][:80], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_and_retrieve_topk.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, faiss, numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= è·¯å¾„é…ç½® =========\n",
    "# å·²å»ºå¥½çš„å‘é‡åº“ï¼ˆç”± casebank_*_with_embeddings.json æ„å»ºï¼‰\n",
    "FAISS_INDEX_PATH = \"casebank_A_qwen_cls_cosine.index\"\n",
    "ID_MAP_PATH      = \"casebank_A_qwen_cls_idmap.json\"\n",
    "\n",
    "# å¾…å‘é‡åŒ–å¹¶æ£€ç´¢çš„æµ‹è¯•é›†\n",
    "TEST_INPUT          = \"picked_balanced_around30.json\"\n",
    "TEST_WITH_EMB_OUT   = \"A_test_with_embeddings_final1.json\"\n",
    "TOPK_RESULTS_OUT    = \"A_test_top1_final1.json\"\n",
    "\n",
    "# Qwen + LoRAï¼ˆä¸å»ºåº“æ—¶å®Œå…¨ä¸€è‡´ï¼‰\n",
    "BASE_MODEL  = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_DIR = \"mbti_lora_qwen1.5b-split_kaggle_ckpt\"\n",
    "\n",
    "TOPK = 1\n",
    "MAX_LEN = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========= åŠ è½½ç´¢å¼• =========\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "id_map = json.load(open(ID_MAP_PATH, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# ========= åŠ è½½æ¨¡å‹ï¼ˆä¸å»ºåº“æ—¶ä¸€è‡´ï¼‰=========\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "cfg.num_labels = 16\n",
    "\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, config=cfg, trust_remote_code=True,\n",
    "    device_map={\"\": device}, low_cpu_mem_usage=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False).eval().to(device)\n",
    "\n",
    "# ========= å·¥å…·å‡½æ•° =========\n",
    "def pick_text(item: dict) -> str:\n",
    "    \"\"\"æŒ‰ä¼˜å…ˆçº§é€‰ä¸€ä¸ªå¯ç”¨äºç¼–ç çš„æ–‡æœ¬å­—æ®µã€‚æŒ‰ä½ çš„æ•°æ®ç»“æ„éœ€è¦å¯è°ƒæ•´é¡ºåºã€‚\"\"\"\n",
    "    for key in [\"posts_cleaned_for_embedding\", \"embed_text\", \"post_casebank\",\n",
    "                \"posts_cleaned\", \"posts\", \"text\", \"query_text\"]:\n",
    "        v = item.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return \"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_to_vec(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ç”¨ masked mean pooling æå¥å‘é‡ï¼ˆæ•´æ®µæ–‡æœ¬è¡¨å¾ï¼‰å¹¶åš L2 å½’ä¸€åŒ–ï¼Œ\n",
    "    ä¸ç´¢å¼•ç«¯ IndexFlatIPï¼ˆå†…ç§¯â‰ˆä½™å¼¦ï¼‰ä¸€è‡´ã€‚\n",
    "    \"\"\"\n",
    "    ins = tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n",
    "    out = model.base_model.model(**ins, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "    last_hidden = out.hidden_states[-1]                 # (B, T, H)\n",
    "    mask = ins[\"attention_mask\"].unsqueeze(-1).float()  # (B, T, 1)\n",
    "\n",
    "    masked = last_hidden * mask                         # zero out pads\n",
    "    sum_hidden = masked.sum(dim=1)                      # (B, H)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1.0)            # (B, 1)\n",
    "    vec = sum_hidden / lengths                          # (B, H)\n",
    "\n",
    "    vec = F.normalize(vec, p=2, dim=1)                  # L2 å½’ä¸€åŒ–\n",
    "    return vec.squeeze(0).float().cpu().numpy()         # (H,)\n",
    "\n",
    "def search_topk(vec: np.ndarray, k=TOPK):\n",
    "    D, I = index.search(vec.reshape(1, -1), k)\n",
    "    return [{\"score\": float(s), **id_map[str(int(i))]} for s, i in zip(D[0], I[0])]\n",
    "\n",
    "def filter_self_hits(query_text: str, hits: list, k: int):\n",
    "    \"\"\"å¯é€‰ï¼šè¿‡æ»¤ä¸æŸ¥è¯¢æ–‡æœ¬å®Œå…¨ç›¸åŒçš„è¿”å›ï¼Œé¿å…â€œå‘½ä¸­è‡ªå·±â€\"\"\"\n",
    "    def norm(s): return \" \".join((s or \"\").lower().split())\n",
    "    qt = norm(query_text)\n",
    "    keep = [h for h in hits if norm(h.get(\"post_casebank\", \"\")) != qt]\n",
    "    return keep[:k]\n",
    "\n",
    "# ========= è¯»å–æµ‹è¯•é›†ã€ç¼–ç ã€æ£€ç´¢ =========\n",
    "test = json.load(open(TEST_INPUT, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "with_emb, results = [], []\n",
    "for it in tqdm(test, desc=\"Encode & retrieve\"):\n",
    "    text = pick_text(it)\n",
    "    if not text:\n",
    "        # æ²¡æœ‰å¯ç”¨æ–‡æœ¬å°±è®°ç©º\n",
    "        it_out = dict(it); it_out[\"embedding\"] = None\n",
    "        with_emb.append(it_out)\n",
    "        results.append({\"type\": it.get(\"type\",\"\"), \"query_text\": \"\", \"topk_cases\": []})\n",
    "        continue\n",
    "\n",
    "    emb = encode_text_to_vec(text)              # (H,)\n",
    "    it_out = dict(it); it_out[\"embedding\"] = emb.tolist()\n",
    "    with_emb.append(it_out)\n",
    "\n",
    "    topk = search_topk(emb, k=TOPK)\n",
    "    topk = filter_self_hits(text, topk, TOPK)   # å¯å…³æ‰ï¼šå¦‚æœä¸æ‹…å¿ƒå‘½ä¸­è‡ªå·±\n",
    "    results.append({\n",
    "        \"type\": it.get(\"type\",\"\"),\n",
    "        \"query_text\": text,\n",
    "        \"topk_cases\": topk\n",
    "    })\n",
    "\n",
    "# ========= è½ç›˜ =========\n",
    "json.dump(with_emb, open(TEST_WITH_EMB_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "json.dump(results, open(TOPK_RESULTS_OUT, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åŒ–å®Œæˆï¼š\", TEST_WITH_EMB_OUT)\n",
    "print(\"âœ… TopK æ£€ç´¢å®Œæˆï¼š\", TOPK_RESULTS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7e483-c975-471a-b6aa-ee0725a7c36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== æ¨¡å‹é…ç½® =====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MODEL_PATH = \"best_fem_deberta.pt\"\n",
    "USE_CLS = True\n",
    "MIN_WORDS = 5 # æœ€å°è¯æ•°è¦æ±‚\n",
    "\n",
    "# ===== åŠ è½½æ¨¡å‹å’Œ tokenizer =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ===== æå–åµŒå…¥å‘é‡å‡½æ•° =====\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        if USE_CLS:\n",
    "            return outputs.last_hidden_state[:, 0, :].squeeze().cpu().tolist()\n",
    "        else:\n",
    "            return torch.mean(outputs.last_hidden_state, dim=1).squeeze().cpu().tolist()\n",
    "\n",
    "# ===== è¯»å–åŸå§‹ casebank æ•°æ® =====\n",
    "with open(\"casebank_mbti.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "output = []\n",
    "kept_count = 0\n",
    "filtered_count = 0\n",
    "\n",
    "for item in tqdm(data, desc=\"ğŸ” Extracting embeddings\"):\n",
    "    post_text = item.get(\"post_casebank\", \"\").strip()\n",
    "    mbti_type = item.get(\"type\", \"\")\n",
    "\n",
    "    # ===== è¿‡æ»¤çŸ­æ–‡æœ¬ =====\n",
    "    word_count = len(post_text.split())\n",
    "    if word_count < MIN_WORDS:\n",
    "        filtered_count += 1\n",
    "        continue\n",
    "\n",
    "    emb = get_embedding(post_text)\n",
    "    output.append({\n",
    "        \"embedding\": emb,\n",
    "        \"post_casebank\": post_text,\n",
    "        \"type\": mbti_type\n",
    "    })\n",
    "    kept_count += 1\n",
    "\n",
    "# ===== ä¿å­˜ =====\n",
    "output_file = \"mbti_embeddings_from_custom_model.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ===== æ‰“å°ç»Ÿè®¡ç»“æœ =====\n",
    "print(\"âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š\", output_file)\n",
    "print(f\"ğŸ“Š å…±å¤„ç†æ ·æœ¬æ•°ï¼š{len(data)}\")\n",
    "print(f\"âœ… ä¿ç•™æ ·æœ¬æ•°ï¼š{kept_count}\")\n",
    "print(f\"ğŸš« ç­›é™¤è¿‡çŸ­æ ·æœ¬æ•°ï¼š{filtered_count}ï¼ˆ<{MIN_WORDS}è¯ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bca73f-5a51-4e12-823a-daa20b58e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ========== é…ç½® ==========\n",
    "DATA_PATH = \"mbti_embeddings_from_custom_model.json\"\n",
    "FAISS_INDEX_PATH = \"mbti_faiss_cosine.index\"\n",
    "ID_MAP_PATH = \"mbti_idmap_cosine.json\"\n",
    "\n",
    "# ========== åŠ è½½åµŒå…¥ ==========\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "embeddings = []\n",
    "id_map = {}\n",
    "\n",
    "for idx, item in enumerate(raw_data):\n",
    "    emb = np.array(item[\"embedding\"], dtype=np.float32)\n",
    "    embeddings.append(emb)\n",
    "    id_map[idx] = {\n",
    "        \"post_casebank\": item[\"post_casebank\"],\n",
    "        \"type\": item[\"type\"]\n",
    "    }\n",
    "\n",
    "# ========== å½’ä¸€åŒ– + æ„å»ºä½™å¼¦ç›¸ä¼¼åº¦ç´¢å¼• ==========\n",
    "embeddings = np.vstack(embeddings)\n",
    "faiss.normalize_L2(embeddings)  # ğŸ‘ˆ å½’ä¸€åŒ–åå†…ç§¯å°±ç­‰æ•ˆäºä½™å¼¦ç›¸ä¼¼åº¦\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ï¼‰\n",
    "index.add(embeddings)\n",
    "\n",
    "# ========== ä¿å­˜ ==========\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "with open(ID_MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡æ•°æ®åº“ï¼ˆCosine ç›¸ä¼¼åº¦ï¼‰æ„å»ºå®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce251e6e-f08f-4f89-bcda-ee33240923c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œï¼Œéœ€è¦å–æ¶ˆæ³¨é‡Šï¼š\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# âœ… åµŒå…¥æ¨¡å‹æ¸…æ´—ï¼šæ ‡å‡† NLP æ¸…æ´—\n",
    "def clean_for_embedding(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = text.replace('|', ' ')\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# âœ… å¤§æ¨¡å‹æ¸…æ´—ï¼šåªå» URL å’Œ |\n",
    "def clean_for_llm(text):\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = text.replace('|', ' ')\n",
    "    return text.strip()\n",
    "\n",
    "# ä¸»å¤„ç†å‡½æ•°\n",
    "def preprocess_dual_clean(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    cleaned_data = []\n",
    "    for item in tqdm(raw_data, desc=\"Cleaning Dual Versions\"):\n",
    "        post = item[\"posts\"]\n",
    "        cleaned_data.append({\n",
    "            \"type\": item[\"type\"],\n",
    "            \"posts\": post,\n",
    "            \"posts_cleaned_for_embedding\": clean_for_embedding(post),\n",
    "            \"posts_cleaned_for_llm\": clean_for_llm(post)\n",
    "        })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… åŒé‡æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜è‡³ {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dual_clean(\"filtered_processed_comments_cleaned_random50.json\", \"pandora_cleaned.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f42a84-f3b0-46ab-af13-c954bf8dcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#éªŒè¯æ•°æ®åšembedding\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MODEL_PATH = \"best_fem_deberta.pt\"\n",
    "USE_CLS = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "# === è¯»å–æ¸…æ´—åçš„éªŒè¯é›† ===\n",
    "with open(\"pandora_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "results = []\n",
    "for item in tqdm(data, desc=\"Embedding\"):\n",
    "    emb = get_embedding(item[\"posts_cleaned_for_embedding\"])\n",
    "    results.append({\n",
    "        \"type\": item[\"type\"],\n",
    "        \"embedding\": emb.tolist(),\n",
    "        \"posts_cleaned_for_llm\": item[\"posts_cleaned_for_llm\"],\n",
    "        \"posts_cleaned_for_embedding\": item[\"posts_cleaned_for_embedding\"]\n",
    "    })\n",
    "\n",
    "with open(\"pandora_with_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… å‘é‡åŒ–å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e19f16-40a5-49bb-a7bd-02e8a5f2e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¹éªŒè¯æ•°æ®è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆTop-Kï¼‰å¹¶å‡†å¤‡ç»™ LLM ä½¿ç”¨çš„å¯¹è¯ä¸Šä¸‹æ–‡ç»“æ„\n",
    "\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# è¯»å– FAISS å’Œ ID æ˜ å°„\n",
    "index = faiss.read_index(\"mbti_faiss.index\")\n",
    "with open(\"mbti_idmap.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    id_map = json.load(f)\n",
    "\n",
    "def retrieve_topk(query_embedding, k=5):\n",
    "    query_embedding = np.array(query_embedding).astype(np.float32).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return [id_map[str(i)] for i in I[0]]\n",
    "\n",
    "# è¯»å–éªŒè¯é›†å¸¦ embedding çš„æ•°æ®\n",
    "with open(\"pandora_with_embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    verification_data = json.load(f)\n",
    "\n",
    "retrieved_results = []\n",
    "\n",
    "for item in verification_data:\n",
    "    query_emb = item[\"embedding\"]\n",
    "    topk_cases = retrieve_topk(query_emb, k=3)\n",
    "    retrieved_results.append({\n",
    "        \"type\": item[\"type\"],\n",
    "        \"posts_cleaned_for_llm\": item[\"posts_cleaned_for_llm\"],\n",
    "        \"posts_cleaned_for_embedding\": item[\"posts_cleaned_for_embedding\"],\n",
    "        \"topk_cases\": topk_cases\n",
    "    })\n",
    "\n",
    "with open(\"retrieved_results_for_llm_pandora.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(retrieved_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… æ£€ç´¢å®Œæˆï¼ç»“æœä¿å­˜åœ¨ retrieved_results_for_llm_pandora.json\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"retrieved_results_for_llm_pandora.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # å°å¿ƒâš ï¸ï¼šå¤§æ–‡ä»¶å®¹æ˜“çˆ†å†…å­˜\n",
    "\n",
    "# åªçœ‹å‰ 5 æ¡\n",
    "for i, item in enumerate(data[:5]):\n",
    "    print(f\"\\n=== ç¬¬ {i+1} æ¡æ ·æœ¬ ===\")\n",
    "    print(\"Type:\", item[\"type\"])\n",
    "    print(\"Query Post:\", item[\"posts_cleaned_for_llm\"][:300], \"...\")\n",
    "    print(\"Top-K types:\", [x[\"type\"] for x in item[\"topk_cases\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddac7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json, re, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Extract final MBTI type from LLM output ===\n",
    "def extract_final_type(text):\n",
    "    match = re.search(r'(Final Type|æœ€ç»ˆç±»å‹)[:ï¼š]?\\s*([IEie][NSns][FTft][JPjp])\\b', text)\n",
    "    if match:\n",
    "        return match.group(2).upper()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# === Build the prompt ===\n",
    "def build_prompt(user_post, topk_cases):\n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"[Reference Example {i+1}]\\nPost Content: {ex['post_casebank']}\\nMBTI Type: {ex['type']}\" \n",
    "        for i, ex in enumerate(topk_cases)\n",
    "    ])\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in MBTI personality typing and linguistic style analysis.\n",
    "\n",
    "[User Post]\n",
    "{user_post}\n",
    "\n",
    "[Reference Examples]\n",
    "{examples_text}\n",
    "\n",
    "---\n",
    "1. Final Type: ______\n",
    "2. Analyze the writing style, tone, logicality, and emotionality.\n",
    "3. Compare it with each reference example and explain similarities.\n",
    "4. Conclude with the most likely MBTI type.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# === LLM API wrapper ===\n",
    "class LLM_API_Wrapper:\n",
    "    def __init__(self, model, api_key):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def call_api(self, prompt_text: str, n: int = 1):\n",
    "        url = \"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": 0.8,\n",
    "            \"n\": n,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            \"max_completion_tokens\": 512,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\"Content-Type\": \"application/json\",\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        try:\n",
    "            r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return [c[\"message\"][\"content\"] for c in result.get(\"choices\", [])]\n",
    "        except Exception as e:\n",
    "            print(\"âŒ Request failed:\", e)\n",
    "            return []\n",
    "\n",
    "# === Call until we collect enough valid responses ===\n",
    "def robust_llm_call(llm, prompt, vote_times=40, n_valid=20, max_total_requests=20):\n",
    "    all_valid_responses, total_requests = [], 0\n",
    "    while len(all_valid_responses) < n_valid and total_requests < max_total_requests:\n",
    "        total_requests += 1\n",
    "        new_responses = llm.call_api(prompt, n=vote_times)\n",
    "        valid = [r for r in new_responses if extract_final_type(r) != \"UNKNOWN\"]\n",
    "        all_valid_responses.extend(valid)\n",
    "        print(f\"ğŸ”„ Collected {len(all_valid_responses)} valid responses...\")\n",
    "    return all_valid_responses[:n_valid]\n",
    "\n",
    "# === Main function: CoT-SC majority voting ===\n",
    "def evaluate_with_voting(input_file, output_file, llm: LLM_API_Wrapper, vote_times=40, n_valid=20):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_inputs = json.load(f)\n",
    "\n",
    "    outputs, correct, total = [], 0, 0\n",
    "    type_correct, type_total = defaultdict(int), defaultdict(int)\n",
    "\n",
    "    for item in tqdm(all_inputs, desc=\"ğŸ” Running CoT-SC voting\"):\n",
    "        prompt = build_prompt(item[\"query_text\"], item[\"topk_cases\"])\n",
    "        responses = robust_llm_call(llm, prompt, vote_times, n_valid)\n",
    "        if responses:\n",
    "            predicted_types = [extract_final_type(r) for r in responses]\n",
    "            majority_vote = Counter(predicted_types).most_common(1)[0][0]\n",
    "            ground_truth = item.get(\"type\", \"UNKNOWN\").upper()\n",
    "            is_correct = majority_vote == ground_truth\n",
    "            outputs.append({\n",
    "                \"query_text\": item[\"query_text\"],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"final_prediction\": majority_vote,\n",
    "                \"correct\": is_correct,\n",
    "                \"llm_responses\": responses\n",
    "            })\n",
    "            if ground_truth != \"UNKNOWN\":\n",
    "                total += 1; type_total[ground_truth]+=1\n",
    "                if is_correct: correct+=1; type_correct[ground_truth]+=1\n",
    "        else:\n",
    "            outputs.append({\"query_text\": item[\"query_text\"],\"final_prediction\":\"ERROR\",\"correct\":False})\n",
    "    json.dump(outputs, open(output_file,\"w\",encoding=\"utf-8\"),ensure_ascii=False,indent=2)\n",
    "    print(f\"âœ… Voting finished. Results saved to {output_file}\")\n",
    "    if total>0:\n",
    "        print(f\"ğŸ¯ Accuracy: {correct}/{total} = {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "with open(\"testå¯¹åº”çš„åŸå§‹æ•°æ®.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cnt = Counter([d.get(\"type\", \"UNKNOWN\").upper() for d in data])\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT_FILE  = \"QwenBase_test_topk3.json\"     # è¾“å…¥æ–‡ä»¶\n",
    "OUTPUT_FILE = \"QwenBase_test_topk3_clip32.json\"  # è¾“å‡ºæ–‡ä»¶\n",
    "MAX_PER_TYPE = 32\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "buckets = defaultdict(list)\n",
    "for item in data:\n",
    "    t = item.get(\"type\", \"UNKNOWN\").upper()\n",
    "    if len(buckets[t]) < MAX_PER_TYPE:\n",
    "        buckets[t].append(item)\n",
    "\n",
    "# æ‹¼å›åˆ—è¡¨\n",
    "clipped = []\n",
    "for t, items in buckets.items():\n",
    "    clipped.extend(items)\n",
    "    print(f\"{t}: {len(items)} kept\")\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clipped, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Done! Saved to {OUTPUT_FILE}, total {len(clipped)} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9980aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM_API_Wrapper(\n",
    "    model=\"gpt-4o-mini\",        # or your deployed model name\n",
    "    api_key=\"\" # replace with your actual key\n",
    ")\n",
    "\n",
    "evaluate_with_voting(\n",
    "    input_file=\"B_test_20_topk_clip50.json\",         # your TopK retrieved file\n",
    "    output_file=\"B_test_20_clip50_vote_results.json\",# output file\n",
    "    llm=llm,\n",
    "    vote_times=5,   # how many responses per request\n",
    "    n_valid=5       # how many valid responses needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_mbti_accuracy(result_file):\n",
    "    with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # å››ä¸ªç»´åº¦ç»Ÿè®¡\n",
    "    dim_names = [\"IE\", \"NS\", \"TF\", \"JP\"]\n",
    "    dim_correct = [0, 0, 0, 0]\n",
    "    dim_total = [0, 0, 0, 0]\n",
    "\n",
    "    # å®Œæ•´å››å­—æ¯\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # æ¯ç§ç±»å‹çš„ç»Ÿè®¡\n",
    "    type_total = defaultdict(int)\n",
    "    type_correct = defaultdict(int)\n",
    "\n",
    "    for item in data:\n",
    "        gt = item[\"ground_truth\"].upper()\n",
    "        pred = item[\"final_prediction\"].upper()\n",
    "\n",
    "        if len(gt) == 4 and len(pred) == 4:\n",
    "            total += 1\n",
    "            type_total[gt] += 1\n",
    "\n",
    "            if gt == pred:\n",
    "                correct += 1\n",
    "                type_correct[gt] += 1\n",
    "\n",
    "            # å››ä¸ªç»´åº¦é€ä½æ¯”è¾ƒ\n",
    "            for i in range(4):\n",
    "                dim_total[i] += 1\n",
    "                if gt[i] == pred[i]:\n",
    "                    dim_correct[i] += 1\n",
    "\n",
    "    # è¾“å‡ºç»“æœ\n",
    "    print(\"=== å››ä¸ªç»´åº¦å‡†ç¡®ç‡ ===\")\n",
    "    for name, c, t in zip(dim_names, dim_correct, dim_total):\n",
    "        acc = c / t if t > 0 else 0\n",
    "        print(f\"{name}: {c}/{t} = {acc:.4f}\")\n",
    "\n",
    "    print(\"\\n=== å››å­—æ¯å®Œå…¨åŒ¹é…å‡†ç¡®ç‡ ===\")\n",
    "    print(f\"Overall: {correct}/{total} = {correct/total:.4f}\")\n",
    "\n",
    "    print(\"\\n=== æ¯ä¸ªç±»å‹çš„å‡†ç¡®ç‡ ===\")\n",
    "    all_types = [\n",
    "        \"INTJ\",\"INTP\",\"ENTJ\",\"ENTP\",\n",
    "        \"INFJ\",\"INFP\",\"ENFJ\",\"ENFP\",\n",
    "        \"ISTJ\",\"ISFJ\",\"ESTJ\",\"ESFJ\",\n",
    "        \"ISTP\",\"ISFP\",\"ESTP\",\"ESFP\"\n",
    "    ]\n",
    "    for t in all_types:\n",
    "        c, tt = type_correct[t], type_total[t]\n",
    "        acc = c / tt if tt > 0 else 0\n",
    "        print(f\"{t}: {c}/{tt} = {acc:.4f}\")\n",
    "\n",
    "\n",
    "# è°ƒç”¨\n",
    "evaluate_mbti_accuracy(\"B_test_20_clip50_vote_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2481f4f-11ff-4ba4-8cf7-6448f668d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# === æå–æœ€ç»ˆ MBTI ç±»å‹ ===\n",
    "def extract_final_type(text):\n",
    "    # å…¼å®¹â€œFinal Typeâ€å’Œâ€œæœ€ç»ˆç±»å‹â€ä¸¤ç§å†™æ³•\n",
    "    match = re.search(r'(Final Type|æœ€ç»ˆç±»å‹)[:ï¼š]?\\s*([IEie][NSns][FTft][JPjp])\\b', text)\n",
    "    if match:\n",
    "        return match.group(2).upper()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# === æ„é€  Prompt ===\n",
    "def build_prompt(user_post, topk_cases):\n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"[Reference Example {i+1}]\\nPost Content: {ex['post_casebank']}\\nMBTI Type: {ex['type']}\" \n",
    "        for i, ex in enumerate(topk_cases)\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in MBTI personality typing and linguistic style analysis.\n",
    "\n",
    "Your task is to infer the most likely MBTI type of a user's post **by analyzing their writing style** â€” such as tone, phrasing, language complexity, emotionality, and personality cues â€” and comparing it to several known examples labeled with MBTI types.\n",
    "\n",
    "**You must treat the reference examples as correct and trustworthy.**\n",
    "Do not ignore them â€” they provide strong clues about how different types express themselves.\n",
    "\n",
    "---\n",
    "\n",
    "[User Post]\n",
    "{user_post}\n",
    "\n",
    "[Reference Examples]\n",
    "{examples_text}\n",
    "\n",
    "---\n",
    "\n",
    "Now follow these steps:\n",
    "1.Final Type: ______\n",
    "\n",
    "2. **Analyze** the user's post: What is its tone? Abstract or concrete? Emotional or logical? Structured or scattered?\n",
    "\n",
    "3. **Compare** it with each reference example: Which ones are stylistically most similar to the user? Provide reasons.\n",
    "\n",
    "4. **Conclude** with the MBTI type that best matches the user post, based on the closest stylistic match.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# === å°è£… LLM API æ¥å£ ===\n",
    "class LLM_API_Wrapper:\n",
    "    def __init__(self, model, api_key):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def call_api(self, prompt_text: str, n: int = 1):\n",
    "        url = \"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": 0.8,\n",
    "            \"n\": n,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            \"modalities\": [\"text\"],\n",
    "            \"response_format\": {\"type\": \"text\"},\n",
    "            \"max_completion_tokens\": 512,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            if \"choices\" in result:\n",
    "                return [c[\"message\"][\"content\"] for c in result[\"choices\"]]\n",
    "            else:\n",
    "                print(\"âš ï¸ Missing 'choices' in response.\")\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Request failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# === å¾ªç¯ç›´åˆ°è·å¾— n_valid æ¡æœ‰æ•ˆå“åº” ===\n",
    "def robust_llm_call(llm, prompt, vote_times=40, n_valid=20, max_total_requests=20):\n",
    "    all_valid_responses = []\n",
    "    total_requests = 0\n",
    "\n",
    "    while len(all_valid_responses) < n_valid:\n",
    "        total_requests += 1\n",
    "        if total_requests > max_total_requests:\n",
    "            print(\"âš ï¸ è¯·æ±‚æ¬¡æ•°è¿‡å¤šï¼Œè·³è¿‡è¯¥æ ·æœ¬\")\n",
    "            return []\n",
    "\n",
    "        new_responses = llm.call_api(prompt, n=vote_times)\n",
    "        if not new_responses:\n",
    "            continue\n",
    "\n",
    "        valid = [r for r in new_responses if extract_final_type(r) != \"UNKNOWN\"]\n",
    "        all_valid_responses.extend(valid)\n",
    "\n",
    "        print(f\"ğŸ”„ å·²ç´¯è®¡ {len(all_valid_responses)} æ¡æœ‰æ•ˆå›ç­”...\")\n",
    "\n",
    "    return all_valid_responses[:n_valid]\n",
    "\n",
    "# === ä¸»å‡½æ•°ï¼šCoT-SC å¤šè½®æŠ•ç¥¨é¢„æµ‹ ===\n",
    "def evaluate_with_voting(input_file, output_file, llm: LLM_API_Wrapper, vote_times=40, n_valid=20):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_inputs = json.load(f)\n",
    "\n",
    "    outputs = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    type_correct = defaultdict(int)\n",
    "    type_total = defaultdict(int)\n",
    "\n",
    "    for item in tqdm(all_inputs, desc=\"ğŸ” æ­£åœ¨è¿›è¡Œ CoT-SC é¢„æµ‹\"):\n",
    "        prompt = build_prompt(item[\"query_post\"], item[\"topk_cases\"])\n",
    "        responses = robust_llm_call(llm, prompt, vote_times, n_valid)\n",
    "\n",
    "        if responses:\n",
    "            predicted_types = [extract_final_type(r) for r in responses]\n",
    "            majority_vote = Counter(predicted_types).most_common(1)[0][0]\n",
    "            ground_truth = item.get(\"type\", \"UNKNOWN\").upper()\n",
    "            is_correct = majority_vote == ground_truth\n",
    "\n",
    "            outputs.append({\n",
    "                \"query_post\": item[\"query_post\"],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"llm_responses\": responses,\n",
    "                \"predicted_types\": predicted_types,\n",
    "                \"final_prediction\": majority_vote,\n",
    "                \"correct\": is_correct,\n",
    "                \"prompt\": prompt\n",
    "            })\n",
    "\n",
    "            if ground_truth != \"UNKNOWN\":\n",
    "                total += 1\n",
    "                type_total[ground_truth] += 1\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                    type_correct[ground_truth] += 1\n",
    "        else:\n",
    "            outputs.append({\n",
    "                \"query_post\": item[\"query_post\"],\n",
    "                \"ground_truth\": item.get(\"type\", \"UNKNOWN\"),\n",
    "                \"llm_responses\": [],\n",
    "                \"predicted_types\": [],\n",
    "                \"final_prediction\": \"ERROR\",\n",
    "                \"correct\": False,\n",
    "                \"prompt\": prompt\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(outputs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… æŠ•ç¥¨é¢„æµ‹å®Œæˆï¼Œç»“æœä¿å­˜è‡³ {output_file}\")\n",
    "    if total > 0:\n",
    "        acc = correct / total\n",
    "        print(f\"\\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡ï¼š{acc:.4f}ï¼ˆ{correct} / {total}ï¼‰\")\n",
    "        print(\"\\nğŸ“Š å„ç±»å‹å‡†ç¡®ç‡ï¼š\")\n",
    "        for mbti_type in sorted(type_total.keys()):\n",
    "            total_count = type_total[mbti_type]\n",
    "            correct_count = type_correct[mbti_type]\n",
    "            acc_type = correct_count / total_count\n",
    "            print(f\"{mbti_type}: {acc_type:.2f} ({correct_count} / {total_count})\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ²¡æœ‰åŒ…å« ground_truth çš„æ•°æ®ï¼Œæ— æ³•è®¡ç®—å‡†ç¡®ç‡ã€‚\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ade41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"retrieved_results_for_llm2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# æå–æ‰€æœ‰çš„ç±»å‹\n",
    "types = [item[\"type\"].upper() for item in data if \"type\" in item]\n",
    "\n",
    "# ç»Ÿè®¡æ•°é‡\n",
    "type_counts = Counter(types)\n",
    "\n",
    "# è¾“å‡ºæ¯ç§ç±»å‹çš„æ ·æœ¬æ•°\n",
    "for mbti_type, count in sorted(type_counts.items()):\n",
    "    print(f\"{mbti_type}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM_API_Wrapper(model=\"gpt-4o-mini\", api_key=\"\")\n",
    "\n",
    "evaluate_with_voting(\n",
    "    input_file=\"retrieved_results_for_llm2.json\",\n",
    "    output_file=\"voting_eval_results1.json\",\n",
    "    llm=llm,\n",
    "    vote_times=5,\n",
    "    n_valid=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM_API_Wrapper(model=\"gpt-4o\", api_key=\"\")\n",
    "\n",
    "evaluate_with_voting(\n",
    "    input_file=\"retrieved_results_for_llm_cosine.json\",\n",
    "    output_file=\"voting_eval_results_cosine.json\",\n",
    "    llm=llm,\n",
    "    vote_times=10,\n",
    "    n_valid=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1392ac7-6cf7-4305-8366-fdeaff734431",
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Retain\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ===== 1. æ¨¡å‹åŠ è½½ =====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MODEL_PATH = \"best_fem_deberta.pt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ===== 2. å‘é‡æå–å‡½æ•° =====\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "# ===== 3. åŠ è½½ LLM è¾“å‡ºç»“æœï¼ˆå·²åŒ…å«é¢„æµ‹ç±»å‹ï¼‰=====\n",
    "with open(\"llm_outputs_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# ===== 4. æ„å»º Retain æ ·æœ¬å¹¶æå–å‘é‡ =====\n",
    "new_vectors = []\n",
    "new_id_map = {}\n",
    "new_data = []\n",
    "\n",
    "for i, item in enumerate(results):\n",
    "    query_post = item[\"query_post\"]\n",
    "    posts_cleaned = item.get(\"posts_cleaned\", \"\")\n",
    "    final_pred = item.get(\"final_prediction\", \"UNKNOWN\")\n",
    "    topk_cases = item.get(\"topk_cases\", [])\n",
    "\n",
    "    post_casebank_text = \"|||\".join([case[\"post_casebank\"] for case in topk_cases])\n",
    "\n",
    "    record = {\n",
    "        \"type\": final_pred,\n",
    "        \"posts\": query_post,\n",
    "        \"posts_cleaned\": posts_cleaned,\n",
    "        \"post_casebank\": post_casebank_text\n",
    "    }\n",
    "\n",
    "    emb = get_embedding(post_casebank_text)\n",
    "    new_vectors.append(emb.astype(np.float32))\n",
    "    new_id_map[str(i)] = record\n",
    "    new_data.append(record)\n",
    "\n",
    "# ===== 5. åŠ è½½åŸ FAISS index + id_map å¹¶æ›´æ–° =====\n",
    "index = faiss.read_index(\"mbti_faiss.index\")\n",
    "with open(\"mbti_idmap.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    id_map = json.load(f)\n",
    "\n",
    "start_id = len(id_map)\n",
    "for i, emb in enumerate(new_vectors):\n",
    "    index.add(np.array([emb]))  # æ·»åŠ æ–°å‘é‡\n",
    "    id_map[str(start_id + i)] = new_id_map[str(i)]\n",
    "\n",
    "# ===== 6. ä¿å­˜æ›´æ–°åçš„ index å’Œ id_map =====\n",
    "faiss.write_index(index, \"mbti_faiss.index\")\n",
    "with open(\"mbti_idmap.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(id_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Retain å®Œæˆï¼šå·²æ·»åŠ  {len(new_vectors)} æ¡æ ·æœ¬è‡³å‘é‡æ•°æ®åº“ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–ä½ çš„ LLM æ¥å£å°è£…ï¼ˆæ›¿æ¢ä¸ºä½ çš„çœŸå® API KEYï¼‰\n",
    "llm = LLM_API_Wrapper(model=\"gpt-4o\", api_key=\"\")\n",
    "\n",
    "# è¿è¡Œè¯„ä¼°ï¼šè¾“å…¥ä¸ºä½ æ„é€ çš„ TopK æ£€ç´¢ç»“æœ\n",
    "evaluate_with_voting(\n",
    "    input_file=\"retrieved_results_for_llm.json\",\n",
    "    output_file=\"voting_eval_results.json\",\n",
    "    llm=llm,\n",
    "    vote_times=5  # å»ºè®®å…ˆç”¨ 5 è½®æµ‹è¯•ï¼Œæ­£å¼è¯„ä¼°ç”¨ 20 æˆ– 40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19708b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–ä½ çš„ LLM æ¥å£å°è£…ï¼ˆæ›¿æ¢ä¸ºä½ çš„çœŸå® API KEYï¼‰\n",
    "llm = LLM_API_Wrapper(model=\"gpt-4o\", api_key=\"\")\n",
    "\n",
    "# è¿è¡Œè¯„ä¼°ï¼šè¾“å…¥ä¸ºä½ æ„é€ çš„ TopK æ£€ç´¢ç»“æœ\n",
    "evaluate_with_voting(\n",
    "    input_file=\"retrieved_results_for_llm.json\",\n",
    "    output_file=\"voting_eval_results.json\",\n",
    "    llm=llm,\n",
    "    vote_times=5  # å»ºè®®å…ˆç”¨ 5 è½®æµ‹è¯•ï¼Œæ­£å¼è¯„ä¼°ç”¨ 20 æˆ– 40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_query_pipeline(raw_text: str, llm: LLM_API_Wrapper, vote_times: int = 5, do_retain=True):\n",
    "    # 1. æ¸…æ´—\n",
    "    posts_cleaned = clean_text(raw_text)\n",
    "\n",
    "    # 2. åµŒå…¥\n",
    "    query_emb = get_embedding(posts_cleaned)\n",
    "\n",
    "    # 3. æ£€ç´¢\n",
    "    topk_cases = retrieve_topk(query_emb, k=5)\n",
    "\n",
    "    # 4. Prompt æ„é€ \n",
    "    prompt = build_prompt(raw_text, topk_cases)\n",
    "\n",
    "    # 5. LLM æ¨ç†\n",
    "    responses = llm.call_api(prompt, n=vote_times)\n",
    "    predicted_types = [extract_final_type(r) for r in responses]\n",
    "    majority_vote = Counter(predicted_types).most_common(1)[0][0]\n",
    "\n",
    "    result = {\n",
    "        \"query_post\": raw_text,\n",
    "        \"posts_cleaned\": posts_cleaned,\n",
    "        \"topk_cases\": topk_cases,\n",
    "        \"llm_responses\": responses,\n",
    "        \"predicted_types\": predicted_types,\n",
    "        \"final_prediction\": majority_vote,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "\n",
    "    # 6. RETAINï¼ˆå¯é€‰ï¼‰\n",
    "    if do_retain:\n",
    "        post_casebank_text = \"|||\".join([case[\"post_casebank\"] for case in topk_cases])\n",
    "        emb = get_embedding(post_casebank_text)\n",
    "\n",
    "        index = faiss.read_index(\"mbti_faiss.index\")\n",
    "        with open(\"mbti_idmap.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            id_map = json.load(f)\n",
    "\n",
    "        new_id = str(len(id_map))\n",
    "        index.add(np.array([emb.astype(np.float32)]))\n",
    "        id_map[new_id] = {\n",
    "            \"post_casebank\": post_casebank_text,\n",
    "            \"type\": majority_vote\n",
    "        }\n",
    "\n",
    "        faiss.write_index(index, \"mbti_faiss.index\")\n",
    "        with open(\"mbti_idmap.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(id_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab082f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_from_json_file(json_path: str, llm: LLM_API_Wrapper, vote_times=5, retain=True):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_results = []\n",
    "    print(f\"\\nğŸ“„ æ­£åœ¨å¤„ç† JSON æ–‡æ¡£ï¼Œå…± {len(data)} æ¡è®°å½•...\\n\")\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        text = item.get(\"posts\", \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        print(f\"ğŸ” ç¬¬ {idx+1}/{len(data)} æ¡ç”¨æˆ·å‘è¨€å¤„ç†ä¸­...\")\n",
    "\n",
    "        result = run_single_query_pipeline(text, llm, vote_times=vote_times, do_retain=retain)\n",
    "\n",
    "        # å¯é€‰ï¼šè®°å½•åŸå§‹æ ‡ç­¾\n",
    "        result[\"ground_truth\"] = item.get(\"type\", \"UNKNOWN\")\n",
    "        all_results.append(result)\n",
    "\n",
    "    output_path = f\"mbti_predictions_from_{os.path.basename(json_path)}\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… å…¨éƒ¨å®Œæˆï¼é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
