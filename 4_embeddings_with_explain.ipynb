{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e65d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== API Wrapper =====\n",
    "class LLM_API_Wrapper:\n",
    "    def __init__(self, model, api_key):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def call_api(self, prompt_text: str, n: int = 1, timeout_s: int = 90):\n",
    "        url = \"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": 0.6,                 # ç•¥é™æ¸©ï¼Œæå‡ç»“æ„åŒ–è¾“å‡ºæ¦‚ç‡\n",
    "            \"n\": n,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            \"modalities\": [\"text\"],\n",
    "            \"response_format\": {\"type\": \"text\"},\n",
    "            \"max_completion_tokens\": 512,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(url, headers=headers, json=payload, timeout=timeout_s)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            if \"choices\" in data and data[\"choices\"]:\n",
    "                return [c[\"message\"][\"content\"] for c in data[\"choices\"]]\n",
    "            print(\"âš ï¸ Missing 'choices' in response.\")\n",
    "            return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Request failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# ===== Prompt Builders =====\n",
    "def build_full_analysis_prompt(text: str) -> str:\n",
    "    # ä¸¥æ ¼è¦æ±‚åªè¿”å› JSON\n",
    "    return f\"\"\"\n",
    "You are a psycholinguistics expert. Analyze the following social media post from three perspectives:\n",
    "\n",
    "1) Semantic Summary: main idea or intention.\n",
    "2) Sentiment Analysis: emotions/attitudes.\n",
    "3) Linguistic Style: writing style (e.g., emotional, rational, informal, formal, vague).\n",
    "\n",
    "Return ONLY valid JSON with the exact keys below and no extra text:\n",
    "\n",
    "{{\n",
    "  \"semantic_view\": \"...\",\n",
    "  \"sentiment_view\": \"...\",\n",
    "  \"linguistic_view\": \"...\"\n",
    "}}\n",
    "\n",
    "Post:\n",
    "\\\"\\\"\\\"{(text or '').strip()[:1024]}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_fallback_prompt(text: str) -> str:\n",
    "    # é€€è€Œæ±‚å…¶æ¬¡çš„æ›´ç®€å•æç¤º\n",
    "    return f\"\"\"\n",
    "Provide a STRICT JSON object with three short fields summarizing the post:\n",
    "\n",
    "{{\n",
    "  \"semantic_view\": \"<1-2 sentences>\",\n",
    "  \"sentiment_view\": \"<one or two emotions>\",\n",
    "  \"linguistic_view\": \"<style words>\"\n",
    "}}\n",
    "\n",
    "Post: {(text or '').strip()[:1024]}\n",
    "\"\"\".strip()\n",
    "\n",
    "# ===== JSON Parser =====\n",
    "def safe_extract_json(raw: str) -> dict:\n",
    "    \"\"\"\n",
    "    å°½å¯èƒ½ä»å¤šç§å¸¸è§è¿”å›æ ¼å¼ä¸­æå– JSONï¼›ä¸åˆæ³•è¿”å› Noneã€‚\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return None\n",
    "    txt = raw.strip()\n",
    "\n",
    "    # å»æ‰ä»£ç å—åŒ…è£¹\n",
    "    if txt.startswith(\"```\"):\n",
    "        txt = re.sub(r\"^```(json)?\", \"\", txt, flags=re.IGNORECASE).strip()\n",
    "        if txt.endswith(\"```\"):\n",
    "            txt = txt[:-3].strip()\n",
    "\n",
    "    # å¦‚æœå‰åæœ‰å†—ä½™æ–‡å­—ï¼Œå°è¯•æŠ½å–ç¬¬ä¸€ä¸ª {...} å—\n",
    "    # å…è®¸æœ‰æ¢è¡Œ/ç©ºç™½\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", txt)\n",
    "    if m:\n",
    "        txt = m.group(0)\n",
    "\n",
    "    # å•å¼•å· -> åŒå¼•å·ï¼ˆå°½é‡ä¸è¯¯ä¼¤ï¼‰\n",
    "    if txt.count('\"') == 0 and txt.count(\"'\") > 0:\n",
    "        txt = txt.replace(\"â€™\", \"'\").replace(\"â€˜\", \"'\")\n",
    "        txt = re.sub(r\"(?<!\\\\)'\", '\"', txt)\n",
    "\n",
    "    # å»æ‰å°¾éƒ¨å¤šä½™é€—å·\n",
    "    txt = re.sub(r\",\\s*}\", \"}\", txt)\n",
    "    txt = re.sub(r\",\\s*]\", \"]\", txt)\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        # æ ¡éªŒå­—æ®µ\n",
    "        if not isinstance(obj, dict):\n",
    "            return None\n",
    "        for k in (\"semantic_view\", \"sentiment_view\", \"linguistic_view\"):\n",
    "            if k not in obj or not isinstance(obj[k], str):\n",
    "                return None\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ===== å…œåº•ç”Ÿæˆï¼ˆç¡®ä¿æ¯æ¡éƒ½æœ‰ï¼‰=====\n",
    "def fallback_views_from_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    ä¸ä¾èµ– LLM çš„å…œåº•ï¼šç”Ÿæˆç®€çŸ­å¯ç”¨çš„ä¸‰æ®µå­—ç¬¦ä¸²ï¼Œç¡®ä¿å­—æ®µä¸ä¸ºç©ºã€‚\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    # è¯­ä¹‰ï¼šæˆªå–å‰ä¸€å¥æˆ–å‰ 120 å­—ç¬¦\n",
    "    semantic = \"\"\n",
    "    if text:\n",
    "        sentence = re.split(r\"(?<=[.!?ã€‚ï¼ï¼Ÿ])\\s+\", text)[0]\n",
    "        semantic = sentence[:180]\n",
    "    if not semantic:\n",
    "        semantic = \"The post contains limited context; the main idea is unclear.\"\n",
    "\n",
    "    # æƒ…æ„Ÿï¼šå…³é”®è¯æç®€åˆ¤æ–­\n",
    "    lowered = text.lower()\n",
    "    if any(w in lowered for w in [\"love\", \"great\", \"happy\", \"excited\", \"enjoy\", \"å–œæ¬¢\", \"é«˜å…´\", \"å¼€å¿ƒ\"]):\n",
    "        senti = \"Positive, optimistic.\"\n",
    "    elif any(w in lowered for w in [\"hate\", \"angry\", \"sad\", \"tired\", \"annoyed\", \"è®¨åŒ\", \"ç”Ÿæ°”\", \"éš¾è¿‡\", \"ç–²æƒ«\"]):\n",
    "        senti = \"Negative, possibly frustrated or tired.\"\n",
    "    else:\n",
    "        senti = \"Neutral or mixed.\"\n",
    "\n",
    "    # è¯­è¨€é£æ ¼ï¼šæ ¹æ®è¡¨æƒ…/å¤§å°å†™/æ„Ÿå¹å·ç²—ç•¥åˆ¤æ–­\n",
    "    style_tokens = []\n",
    "    if re.search(r\"[A-Z]{3,}\", text): style_tokens.append(\"emphatic\")\n",
    "    if re.search(r\"[!ï¼]{1,}\", text): style_tokens.append(\"expressive\")\n",
    "    if re.search(r\":[)D]|ğŸ˜‚|ğŸ¤£|ğŸ˜…|ğŸ™‚|ğŸ˜‰\", text): style_tokens.append(\"informal\")\n",
    "    if not style_tokens: style_tokens = [\"conversational\"]\n",
    "    ling = \", \".join(style_tokens)\n",
    "\n",
    "    return {\n",
    "        \"semantic_view\": semantic,\n",
    "        \"sentiment_view\": senti,\n",
    "        \"linguistic_view\": ling\n",
    "    }\n",
    "\n",
    "# ===== ä¸»æµç¨‹è®¾ç½® =====\n",
    "MODEL_NAME = \"gpt-4.1-mini\"\n",
    "API_KEY = \"\"               # <<< ç”¨ä½ è‡ªå·±çš„ key\n",
    "MAX_PER_TYPE = 200\n",
    "INPUT_FILE = \"extended_mbti_dataset_v17.json\"\n",
    "OUTPUT_FILE = \"mbti_sample_with_all_views.json\"\n",
    "MAX_RETRY = 5                                # æœ€å¤šé‡è¯• 5 æ¬¡\n",
    "BASE_SLEEP = 2                               # åˆå§‹é€€é¿ç§’\n",
    "\n",
    "# ===== Init =====\n",
    "llm = LLM_API_Wrapper(model=MODEL_NAME, api_key=API_KEY)\n",
    "\n",
    "# ===== Load data =====\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# ===== Balance by type =====\n",
    "type_counter = defaultdict(int)\n",
    "selected_samples = []\n",
    "for item in full_data:\n",
    "    mbti = item[\"type\"]\n",
    "    if MAX_PER_TYPE is None or type_counter[mbti] < MAX_PER_TYPE:\n",
    "        selected_samples.append(item.copy())\n",
    "        type_counter[mbti] += 1\n",
    "    if MAX_PER_TYPE and len(selected_samples) >= 16 * MAX_PER_TYPE:\n",
    "        break\n",
    "\n",
    "print(f\"âœ… Selected {len(selected_samples)} samples ({MAX_PER_TYPE} per MBTI type)\")\n",
    "\n",
    "# ===== Process =====\n",
    "failed_cases = []\n",
    "for i, item in enumerate(tqdm(selected_samples, desc=\"Generating views\")):\n",
    "    post = item.get(\"posts\") or item.get(\"posts_cleaned\") or \"\"\n",
    "    prompt = build_full_analysis_prompt(post)\n",
    "\n",
    "    views = None\n",
    "    for attempt in range(1, MAX_RETRY + 1):\n",
    "        result = llm.call_api(prompt)\n",
    "        parsed = safe_extract_json(result[0]) if result else None\n",
    "\n",
    "        if parsed:\n",
    "            # æ ¡éªŒéç©º\n",
    "            if all(isinstance(parsed[k], str) and parsed[k].strip() for k in parsed):\n",
    "                views = parsed\n",
    "                break\n",
    "            else:\n",
    "                # è¿”å›äº† JSON ä½†å†…å®¹ç©ºï¼Œæ¢æ›´ç®€å•çš„ backup prompt\n",
    "                prompt = build_fallback_prompt(post)\n",
    "        else:\n",
    "            # ç¬¬ä¸€æ¬¡å¤±è´¥åæ”¹ç”¨ fallback promptï¼Œæé«˜æˆåŠŸç‡\n",
    "            prompt = build_fallback_prompt(post)\n",
    "\n",
    "        # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨\n",
    "        sleep_s = BASE_SLEEP * attempt + random.uniform(0, 0.5 * attempt)\n",
    "        print(f\"âš ï¸ Bad/invalid JSON (attempt {attempt}), retry in {sleep_s:.1f}s ...\")\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨å…œåº•ç”Ÿæˆï¼ˆç¡®ä¿æ¯æ¡éƒ½æœ‰ï¼‰\n",
    "    if not views:\n",
    "        print(\"â— Using local fallback generator.\")\n",
    "        views = fallback_views_from_text(post)\n",
    "        failed_cases.append({\"type\": item.get(\"type\"), \"posts\": post})\n",
    "\n",
    "    item[\"semantic_view\"] = views.get(\"semantic_view\", \"\")\n",
    "    item[\"sentiment_view\"] = views.get(\"sentiment_view\", \"\")\n",
    "    item[\"linguistic_view\"] = views.get(\"linguistic_view\", \"\")\n",
    "\n",
    "    # å¯é€‰ï¼šå®šæœŸè½ç›˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘\n",
    "    if (i + 1) % 20 == 0:\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(selected_samples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ===== Save =====\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(selected_samples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Done! Saved enriched samples to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch_optimizer as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_to_binary(mbti_type):\n",
    "    # INFP â†’ [0, 1, 0, 1] (E/I, S/N, T/F, J/P)\n",
    "    return {\n",
    "        \"ei\": 0 if mbti_type[0] == \"I\" else 1,\n",
    "        \"ns\": 0 if mbti_type[1] == \"S\" else 1,\n",
    "        \"tf\": 0 if mbti_type[2] == \"F\" else 1,\n",
    "        \"jp\": 0 if mbti_type[3] == \"P\" else 1\n",
    "    }\n",
    "class JointMBTIDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # åŸºç¡€æ–‡æœ¬\n",
    "        base_text = item.get(\"posts_cleaned\", item[\"posts\"]) or \"\"\n",
    "\n",
    "        # å–ä¸‰è§†è§’ï¼ˆå¦‚æœæ²¡æœ‰å°±ç©ºå­—ç¬¦ä¸²ï¼‰\n",
    "        semantic = item.get(\"semantic_view\", \"\")\n",
    "        sentiment = item.get(\"sentiment_view\", \"\")\n",
    "        linguistic = item.get(\"linguistic_view\", \"\")\n",
    "\n",
    "        # æ‹¼æ¥ï¼šåŸæ–‡ + ä¸‰è§†è§’\n",
    "        combined_text = base_text\n",
    "        if any([semantic, sentiment, linguistic]):\n",
    "            combined_text = f\"{base_text} [SEP] {semantic} [SEP] {sentiment} [SEP] {linguistic}\"\n",
    "\n",
    "        mbti_type = item[\"type\"]\n",
    "        mbti_label = mbti_to_binary(mbti_type)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"ei\": torch.tensor(mbti_label[\"ei\"], dtype=torch.float),\n",
    "            \"ns\": torch.tensor(mbti_label[\"ns\"], dtype=torch.float),\n",
    "            \"tf\": torch.tensor(mbti_label[\"tf\"], dtype=torch.float),\n",
    "            \"jp\": torch.tensor(mbti_label[\"jp\"], dtype=torch.float),\n",
    "            \"label\": mbti_type\n",
    "        }\n",
    "class JointMBTIModel(nn.Module):\n",
    "    def __init__(self, encoder_name=\"microsoft/deberta-v3-base\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier_ei = nn.Linear(hidden_size, 1)\n",
    "        self.classifier_ns = nn.Linear(hidden_size, 1)\n",
    "        self.classifier_tf = nn.Linear(hidden_size, 1)\n",
    "        self.classifier_jp = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = self.dropout(output.last_hidden_state[:, 0, :])\n",
    "        return {\n",
    "            \"embedding\": cls_emb,  # å¯å¯¼å‡ºå‘é‡\n",
    "            \"ei\": self.classifier_ei(cls_emb),\n",
    "            \"ns\": self.classifier_ns(cls_emb),\n",
    "            \"tf\": self.classifier_tf(cls_emb),\n",
    "            \"jp\": self.classifier_jp(cls_emb)\n",
    "        }\n",
    "def compute_mbti_loss(preds, labels):\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    return (\n",
    "        bce(preds[\"ei\"].squeeze(), labels[\"ei\"]) +\n",
    "        bce(preds[\"ns\"].squeeze(), labels[\"ns\"]) +\n",
    "        bce(preds[\"tf\"].squeeze(), labels[\"tf\"]) +\n",
    "        bce(preds[\"jp\"].squeeze(), labels[\"jp\"])\n",
    "    ) / 4\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs=5, save_path=\"kaggle_best_fem.pt\"):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = {k: batch[k].to(device) for k in [\"ei\", \"ns\", \"tf\", \"jp\"]}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = compute_mbti_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        correct_ei = correct_ns = correct_tf = correct_jp = correct_all = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"[Val]   Epoch {epoch+1}\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = {k: batch[k].to(device) for k in [\"ei\", \"ns\", \"tf\", \"jp\"]}\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                pred_ei = torch.sigmoid(outputs[\"ei\"]).round()\n",
    "                pred_ns = torch.sigmoid(outputs[\"ns\"]).round()\n",
    "                pred_tf = torch.sigmoid(outputs[\"tf\"]).round()\n",
    "                pred_jp = torch.sigmoid(outputs[\"jp\"]).round()\n",
    "\n",
    "                correct_ei += (pred_ei.squeeze() == labels[\"ei\"]).sum().item()\n",
    "                correct_ns += (pred_ns.squeeze() == labels[\"ns\"]).sum().item()\n",
    "                correct_tf += (pred_tf.squeeze() == labels[\"tf\"]).sum().item()\n",
    "                correct_jp += (pred_jp.squeeze() == labels[\"jp\"]).sum().item()\n",
    "\n",
    "                correct_all += (\n",
    "                    (pred_ei.squeeze() == labels[\"ei\"]) &\n",
    "                    (pred_ns.squeeze() == labels[\"ns\"]) &\n",
    "                    (pred_tf.squeeze() == labels[\"tf\"]) &\n",
    "                    (pred_jp.squeeze() == labels[\"jp\"])\n",
    "                ).sum().item()\n",
    "\n",
    "                total += input_ids.size(0)\n",
    "\n",
    "        acc_ei = correct_ei / total\n",
    "        acc_ns = correct_ns / total\n",
    "        acc_tf = correct_tf / total\n",
    "        acc_jp = correct_jp / total\n",
    "        acc_all = correct_all / total\n",
    "\n",
    "        print(f\"Validation Accuracy:\")\n",
    "        print(f\"  EI: {acc_ei:.2%} | NS: {acc_ns:.2%} | TF: {acc_tf:.2%} | JP: {acc_jp:.2%}\")\n",
    "        print(f\"  4D Match: {acc_all:.2%}\")\n",
    "\n",
    "        # ä¿å­˜éªŒè¯é›†æœ€ä½³æ¨¡å‹\n",
    "        if acc_all > best_val_acc:\n",
    "            best_val_acc = acc_all\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"âœ… Best model saved to {save_path} (4D match: {acc_all:.2%})\")\n",
    "def visualize_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_embeds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_embeds.append(output[\"embedding\"].cpu())\n",
    "            all_labels.extend(batch[\"label\"])\n",
    "    embeddings = torch.cat(all_embeds).numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "    df = pd.DataFrame(tsne, columns=[\"x\", \"y\"])\n",
    "    df[\"label\"] = all_labels\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in sorted(set(all_labels)):\n",
    "        subset = df[df[\"label\"] == label]\n",
    "        plt.scatter(subset[\"x\"], subset[\"y\"], label=label, alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title(\"FEM Personality Embedding Space (t-SNE)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # åŠ è½½æ•°æ®\n",
    "    with open(\"mbti_sample_with_all_views.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        full_data = json.load(f)\n",
    "\n",
    "    # æ‹†åˆ†è®­ç»ƒé›† / éªŒè¯é›†\n",
    "    train_data, val_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # åˆå§‹åŒ– tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "    # åˆå§‹åŒ– dataset\n",
    "    train_dataset = JointMBTIDataset(train_data, tokenizer)  # æ”¯æŒlist\n",
    "    val_dataset = JointMBTIDataset(val_data, tokenizer)\n",
    "\n",
    "    # åˆå§‹åŒ– dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # æ¨¡å‹å’Œä¼˜åŒ–å™¨\n",
    "    model = JointMBTIModel(\"microsoft/deberta-v3-large\")\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    optimizer = optim.AdamP(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # è®­ç»ƒå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    train_model(model, train_loader, val_loader, optimizer, device, epochs=15)\n",
    "\n",
    "    # å¯è§†åŒ–åµŒå…¥ç©ºé—´ï¼ˆåªç”¨éªŒè¯é›†ï¼‰\n",
    "    visualize_embeddings(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
