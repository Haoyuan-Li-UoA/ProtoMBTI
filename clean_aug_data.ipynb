{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f120cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è¿è¡Œè¯·å–æ¶ˆæ³¨é‡Šè¿™ä¸¤è¡Œä»¥ç¡®ä¿ä¸‹è½½éœ€è¦çš„èµ„æº\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "# åˆå§‹åŒ–å¤„ç†å™¨\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = text.replace('|', ' ')\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "def flatten_nested_json(raw_data):\n",
    "    \"\"\"\n",
    "    å°†å½¢å¦‚ {\"INFP\": [...], \"INTJ\": [...]} çš„å­—å…¸ç»“æ„æ‰“å¹³æˆä¸€ä¸ª list[dict]\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for key in raw_data:\n",
    "        entries = raw_data[key]\n",
    "        if isinstance(entries, list):\n",
    "            for item in entries:\n",
    "                if isinstance(item, dict) and \"post\" in item and \"type\" in item:\n",
    "                    flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "def preprocess_nested_json(input_path, output_path_csv=None, output_path_json=None):\n",
    "    print(f\"ğŸ“¥ åŠ è½½ JSON æ•°æ®: {input_path}\")\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    flat_data = flatten_nested_json(raw_data)\n",
    "\n",
    "    if not flat_data:\n",
    "        raise ValueError(\"âŒ æœªæ‰¾åˆ°åˆæ³•çš„ {'type': ..., 'post': ...} æ•°æ®ç»“æ„\")\n",
    "\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    print(f\"âœ… æ‰å¹³åŒ–å®Œæˆï¼Œå…± {len(df)} æ¡\")\n",
    "\n",
    "    tqdm.pandas(desc=\"Cleaning\")\n",
    "    df[\"posts_cleaned\"] = df[\"post\"].astype(str).progress_apply(clean_text)\n",
    "\n",
    "    # ä¿å­˜ CSV\n",
    "    if output_path_csv:\n",
    "        os.makedirs(os.path.dirname(output_path_csv), exist_ok=True)\n",
    "        df.to_csv(output_path_csv, index=False)\n",
    "        print(f\"âœ… ä¿å­˜ CSV è‡³ï¼š{output_path_csv}\")\n",
    "\n",
    "    # å¯é€‰ï¼šä¿å­˜ä¸º JSON\n",
    "    # å¯é€‰ï¼šä¿å­˜ä¸º JSON\n",
    "    if output_path_json:\n",
    "        os.makedirs(os.path.dirname(output_path_json) or \".\", exist_ok=True)\n",
    "        with open(output_path_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(df.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… ä¿å­˜ JSON è‡³ï¼š{output_path_json}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad04ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ç¤ºä¾‹ç”¨æ³•\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_json = \"augmented_data/all_augmented_data_v17.json\"\n",
    "#     output_csv = \"augmented_data/cleaned_all_augmented_data_with_original_posts_v17.csv\"\n",
    "#     output_json = \"cleaned_all_augmented_data_with_original_posts_v17.json\"\n",
    "\n",
    "#     preprocess_nested_json(input_json, output_path_csv=output_csv, output_path_json=output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b525dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "def preprocess_nested_json(input_path, output_path_csv=None, output_path_json=None):\n",
    "    print(f\"ğŸ“¥ åŠ è½½ JSON æ•°æ®: {input_path}\")\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # åˆ¤æ–­ç»“æ„ï¼šå¦‚æœæ˜¯ list[dict]ï¼Œç›´æ¥å¤„ç†ï¼›å¦‚æœæ˜¯ dictï¼Œå† flatten\n",
    "    if isinstance(raw_data, list):\n",
    "        flat_data = raw_data\n",
    "    elif isinstance(raw_data, dict):\n",
    "        flat_data = flatten_nested_json(raw_data)\n",
    "    else:\n",
    "        raise ValueError(\"âŒ ä¸æ”¯æŒçš„æ•°æ®ç»“æ„ç±»å‹\")\n",
    "\n",
    "    if not flat_data:\n",
    "        raise ValueError(\"âŒ æœªæ‰¾åˆ°åˆæ³•çš„ {'type': ..., 'post': ...} æ•°æ®ç»“æ„\")\n",
    "\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    print(f\"âœ… æ‰å¹³åŒ–å®Œæˆï¼Œå…± {len(df)} æ¡\")\n",
    "\n",
    "    tqdm.pandas(desc=\"Cleaning\")\n",
    "    df[\"posts_cleaned\"] = df[\"posts\"].astype(str).progress_apply(clean_text)\n",
    "\n",
    "    if output_path_csv:\n",
    "        output_dir = os.path.dirname(output_path_csv)\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        df.to_csv(output_path_csv, index=False)\n",
    "        print(f\"âœ… ä¿å­˜ CSV è‡³ï¼š{output_path_csv}\")\n",
    "\n",
    "    if output_path_json:\n",
    "        output_dir = os.path.dirname(output_path_json)\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(output_path_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(df.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… ä¿å­˜ JSON è‡³ï¼š{output_path_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d690574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ åŠ è½½ JSON æ•°æ®: filtered_processed_comments_300.json\n",
      "âœ… æ‰å¹³åŒ–å®Œæˆï¼Œå…± 1261235 æ¡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1261235/1261235 [20:00<00:00, 1050.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¿å­˜ CSV è‡³ï¼šfiltered_processed_comments__300_cleaned.csv\n",
      "âœ… ä¿å­˜ JSON è‡³ï¼šfiltered_processed_comments_300_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"filtered_processed_comments_300.json\"\n",
    "    output_csv = \"filtered_processed_comments__300_cleaned.csv\"\n",
    "    output_json = \"filtered_processed_comments_300_cleaned.json\"\n",
    "\n",
    "    preprocess_nested_json(input_json, output_path_csv=output_csv, output_path_json=output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad13c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ ç¬¬ 1 ä¸ªå­—å…¸ï¼š\n",
      "  type: INTP\n",
      "  posts: What languages do you speak?|||That's authoritarianism versus libertarianism, not left versus right....\n",
      "  posts_cleaned: language speak authoritarianism versus libertarianism left versus right know everyone experience dep...\n",
      "\n",
      "ğŸ”¹ ç¬¬ 2 ä¸ªå­—å…¸ï¼š\n",
      "  type: ENTP\n",
      "  posts: It is just arguing semantics. To many on the consumer end, beta and production phase both share the ...\n",
      "  posts_cleaned: arguing semantics many consumer end beta production phase share fact mainstream release ready intere...\n",
      "\n",
      "ğŸ”¹ ç¬¬ 3 ä¸ªå­—å…¸ï¼š\n",
      "  type: ENTJ\n",
      "  posts: Come to Europe if you're interested. In many EU countries doing a PhD is actually a job, for which y...\n",
      "  posts_cleaned: come europe interested many country phd actually job get paid salary hassle tuition mean bullshit sc...\n",
      "\n",
      "ğŸ”¹ ç¬¬ 4 ä¸ªå­—å…¸ï¼š\n",
      "  type: INTP\n",
      "  posts: Exactly, so I won't have to eat a dick, I knew what I was saying.Two favorite goddesses are Artemis ...\n",
      "  posts_cleaned: exactly eat dick knew saying two favorite goddess artemis athena mean complete package plus shot ath...\n",
      "\n",
      "ğŸ”¹ ç¬¬ 5 ä¸ªå­—å…¸ï¼š\n",
      "  type: ENTP\n",
      "  posts: Ooohhh okay, I thought there was something else to it. Thanks!|||The PS Vita USB port does not have ...\n",
      "  posts_cleaned: ooohhh okay thought something else thanks vita usb port video output doubt still waiting finding pes...\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "\n",
    "filename = 'filtered_processed_comments_cleaned.json'\n",
    "count = 0\n",
    "max_items = 5\n",
    "max_value_length = 100  # æ¯ä¸ªå­—æ®µå€¼æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    parser = ijson.items(f, 'item')\n",
    "\n",
    "    for obj in parser:\n",
    "        print(f\"\\nğŸ”¹ ç¬¬ {count + 1} ä¸ªå­—å…¸ï¼š\")\n",
    "        for key, value in obj.items():\n",
    "            # å­—ç¬¦ä¸²å€¼è£å‰ªï¼Œå…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²åè£å‰ª\n",
    "            val_str = str(value)\n",
    "            if len(val_str) > max_value_length:\n",
    "                val_str = val_str[:max_value_length] + '...'\n",
    "            print(f\"  {key}: {val_str}\")\n",
    "        count += 1\n",
    "        if count >= max_items:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
